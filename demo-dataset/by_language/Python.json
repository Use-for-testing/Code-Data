{
  "language": "Python",
  "samples": [
    {
      "language": "Python",
      "file_path": "Training/Hf-Train-Script.py",
      "content": "#!/usr/bin/env python\n# coding=utf-8\n# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nSupervised fine-tuning script for decoder language models.\n\"\"\"\n\nimport logging\nimport random\nimport sys\n\nimport datasets\nimport torch\nimport transformers\nfrom transformers import AutoModelForCausalLM, set_seed\n\nfrom alignment import (\n    DataArguments,\n    H4ArgumentParser,\n    ModelArguments,\n    SFTConfig,\n    apply_chat_template,\n    decontaminate_humaneval,\n    get_checkpoint,\n    get_datasets,\n    get_kbit_device_map,\n    get_peft_config,\n    get_quantization_config,\n    get_tokenizer,\n)\nfrom trl import SFTTrainer, setup_chat_format\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n    parser = H4ArgumentParser((ModelArguments, DataArguments, SFTConfig))\n    model_args, data_args, training_args = parser.parse()\n\n    # Set seed for reproducibility\n    set_seed(training_args.seed)\n\n    ###############\n    # Setup logging\n    ###############\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n        handlers=[logging.StreamHandler(sys.stdout)],\n    )\n    log_level = training_args.get_process_log_level()\n    logger.setLevel(log_level)\n    datasets.utils.logging.set_verbosity(log_level)\n    transformers.utils.logging.set_verbosity(log_level)\n    transformers.utils.logging.enable_default_handler()\n    transformers.utils.logging.enable_explicit_format()\n\n    # Log on each process a small summary\n    logger.warning(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n    logger.info(f\"Model parameters {model_args}\")\n    logger.info(f\"Data parameters {data_args}\")\n    logger.info(f\"Training/evaluation parameters {training_args}\")\n\n    # Check for last checkpoint\n    last_checkpoint = get_checkpoint(training_args)\n    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n        logger.info(f\"Checkpoint detected, resuming training at {last_checkpoint=}.\")\n\n    ###############\n    # Load datasets\n    ###############\n    raw_datasets = get_datasets(\n        data_args,\n        splits=data_args.dataset_splits,\n        configs=data_args.dataset_configs,\n        columns_to_keep=[\"messages\", \"chosen\", \"rejected\", \"prompt\", \"completion\", \"label\"],\n    )\n    logger.info(\n        f\"Training on the following datasets and their proportions: {[split + ' : ' + str(dset.num_rows) for split, dset in raw_datasets.items()]}\"\n    )\n    column_names = list(raw_datasets[\"train\"].features)\n\n    ################\n    # Load tokenizer\n    ################\n    tokenizer = get_tokenizer(model_args, data_args)\n\n    #######################\n    # Load pretrained model\n    #######################\n    logger.info(\"*** Load pretrained model ***\")\n    torch_dtype = (\n        model_args.torch_dtype if model_args.torch_dtype in [\"auto\", None] else getattr(torch, model_args.torch_dtype)\n    )\n    quantization_config = get_quantization_config(model_args)\n\n    model_kwargs = dict(\n        revision=model_args.model_revision,\n        trust_remote_code=model_args.trust_remote_code,\n        attn_implementation=model_args.attn_implementation,\n        torch_dtype=torch_dtype,\n        use_cache=False if training_args.gradient_checkpointing else True,\n        device_map=get_kbit_device_map() if quantization_config is not None else None,\n        quantization_config=quantization_config,\n    )\n\n    model = model_args.model_name_or_path\n    # For ChatML we need to add special tokens and resize the embedding layer\n    if \"<|im_start|>\" in tokenizer.chat_template and \"gemma-tokenizer-chatml\" not in tokenizer.name_or_path:\n        model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs)\n        model, tokenizer = setup_chat_format(model, tokenizer)\n        model_kwargs = None\n\n    #####################\n    # Apply chat template\n    #####################\n    raw_datasets = raw_datasets.map(\n        apply_chat_template,\n        fn_kwargs={\n            \"tokenizer\": tokenizer,\n            \"task\": \"sft\",\n            \"auto_insert_empty_system_msg\": data_args.auto_insert_empty_system_msg,\n        },\n        num_proc=data_args.preprocessing_num_workers,\n        remove_columns=column_names,\n        desc=\"Applying chat template\",\n    )\n\n    ##########################\n    # Decontaminate benchmarks\n    ##########################\n    num_raw_train_samples = len(raw_datasets[\"train\"])\n    raw_datasets = raw_datasets.filter(decontaminate_humaneval, batched=True, batch_size=10_000, num_proc=1)\n    num_filtered_train_samples = num_raw_train_samples - len(raw_datasets[\"train\"])\n    logger.info(\n        f\"Decontaminated {num_filtered_train_samples} ({num_filtered_train_samples/num_raw_train_samples * 100:.2f}%) samples from the training set.\"\n    )\n\n    train_dataset = raw_datasets[\"train\"]\n    eval_dataset = raw_datasets[\"test\"]\n\n    with training_args.main_process_first(desc=\"Log a few random samples from the processed training set\"):\n        for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n            logger.info(f\"Sample {index} of the processed training set:\\n\\n{raw_datasets['train'][index]['text']}\")\n\n    ########################\n    # Initialize the Trainer\n    ########################\n    trainer = SFTTrainer(\n        model=model,\n        model_init_kwargs=model_kwargs,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        dataset_text_field=\"text\",\n        max_seq_length=training_args.max_seq_length,\n        tokenizer=tokenizer,\n        packing=True,\n        peft_config=get_peft_config(model_args),\n        dataset_kwargs=training_args.dataset_kwargs,\n    )\n\n    ###############\n    # Training loop\n    ###############\n    logger.info(\"*** Train ***\")\n    checkpoint = None\n    if training_args.resume_from_checkpoint is not None:\n        checkpoint = training_args.resume_from_checkpoint\n    elif last_checkpoint is not None:\n        checkpoint = last_checkpoint\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n    metrics = train_result.metrics\n    metrics[\"train_samples\"] = len(train_dataset)\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n    trainer.save_state()\n\n    ##################################\n    # Save model and create model card\n    ##################################\n    logger.info(\"*** Save model ***\")\n    trainer.save_model(training_args.output_dir)\n    logger.info(f\"Model saved to {training_args.output_dir}\")\n\n    # Save everything else on main process\n    kwargs = {\n        \"finetuned_from\": model_args.model_name_or_path,\n        \"dataset\": list(data_args.dataset_mixer.keys()),\n        \"dataset_tags\": list(data_args.dataset_mixer.keys()),\n        \"tags\": [\"alignment-handbook\"],\n    }\n    if trainer.accelerator.is_main_process:\n        trainer.create_model_card(**kwargs)\n        # Restore k,v cache for fast inference\n        trainer.model.config.use_cache = True\n        trainer.model.config.save_pretrained(training_args.output_dir)\n\n    ##########\n    # Evaluate\n    ##########\n    if training_args.do_eval:\n        logger.info(\"*** Evaluate ***\")\n        metrics = trainer.evaluate()\n        metrics[\"eval_samples\"] = len(eval_dataset)\n        trainer.log_metrics(\"eval\", metrics)\n        trainer.save_metrics(\"eval\", metrics)\n\n    if training_args.push_to_hub is True:\n        logger.info(\"Pushing to hub...\")\n        trainer.push_to_hub(**kwargs)\n\n    logger.info(\"*** Training complete ***\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "line_count": 234,
      "code_type": "function",
      "code_name": "main",
      "file_size_bytes": 8318,
      "extraction_timestamp": "2025-05-11T21:41:20.745030"
    },
    {
      "language": "Python",
      "file_path": "Training/Train-Phi3-2bit.py",
      "content": "# ---\n# jupyter:\n#   jupytext:\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: '1.3'\n#       jupytext_version: 1.17.1\n#   kernelspec:\n#     display_name: Python 3 (ipykernel)\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Training Phi-3-mini-128k-instruct to Learn Swift Programming Language\n#\n# This notebook trains Microsoft's Phi-3-mini-128k-instruct model to understand and work with Swift code using a dataset of real Swift files.\n\n# %%\n# Install required libraries\n!pip install transformers datasets evaluate torch scikit-learn tqdm dropbox requests accelerate peft bitsandbytes\n# Set PyTorch memory management environment variables to avoid fragmentation\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Explicitly set to use 2 GPUs\n\n# %%\n# Import required libraries\nimport torch\nimport numpy as np\nimport random\nimport time\nimport collections\nimport psutil\nimport os\nimport gc\nimport json\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    TrainingArguments, \n    Trainer, \n    DataCollatorForLanguageModeling,\n    BitsAndBytesConfig\n)\nfrom transformers.trainer_callback import EarlyStoppingCallback\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n# Import AQLM for 2-bit quantization\ntry:\n    try:\n        import aqlm\n    except ImportError:\n        print(\"AQLM package not found. Installing...\")\n        import subprocess\n        subprocess.check_call([\"pip\", \"install\", \"-U\", \"aqlm\", \"--no-cache-dir\"])\n        import aqlm\n    \n    # AQLM correctly imported - we'll use it directly for quantization when loading the model\n    print(\"AQLM imported successfully - version:\", aqlm.__version__ if hasattr(aqlm, \"__version__\") else \"unknown\")\nexcept Exception as e:\n    print(f\"Error importing AQLM: {e}\")\n    print(\"Will fallback to 4-bit quantization using BitsAndBytes\")\n\n# Define memory cleanup function\ndef cleanup_memory():\n    \"\"\"Clean up GPU memory to avoid fragmentation.\"\"\"\n    print(\"Cleaning up memory...\")\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n        \n# Define resource monitoring function\ndef monitor_resources():\n    \"\"\"Monitor system and GPU resources.\"\"\"\n    # Monitor CPU and RAM\n    process = psutil.Process(os.getpid())\n    memory_info = process.memory_info()\n    print(f\"CPU memory usage: {memory_info.rss / 1024 / 1024:.2f} MB\")\n    \n    # Monitor GPU if available\n    if torch.cuda.is_available():\n        num_gpus = torch.cuda.device_count()\n        print(f\"Number of GPUs: {num_gpus}\")\n        \n        for i in range(num_gpus):\n            if hasattr(torch.cuda, 'memory_allocated'):\n                print(f\"GPU {i} ({torch.cuda.get_device_name(i)})\")\n                print(f\"  Memory allocated: {torch.cuda.memory_allocated(i) / (1024**3):.2f} GB\")\n                print(f\"  Memory reserved: {torch.cuda.memory_reserved(i) / (1024**3):.2f} GB\")\n                if hasattr(torch.cuda, 'memory_stats'):\n                    stats = torch.cuda.memory_stats(i)\n                    if 'active_bytes.all.current' in stats:\n                        print(f\"  Active memory: {stats['active_bytes.all.current'] / (1024**3):.2f} GB\")\n                    if 'reserved_bytes.all.current' in stats:\n                        print(f\"  Reserved memory: {stats['reserved_bytes.all.current'] / (1024**3):.2f} GB\")\n\n\n# %%\n# Check if GPU is available and configure for multi-GPU training\nif torch.cuda.is_available():\n    # Set up for distributed training on multiple GPUs\n    device = torch.device('cuda')\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n    \n    # Enable multi-GPU support for T4 x2\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs\")\n        # For distributed training, we'll use device_map=\"auto\" when loading the model\n        print(\"Multi-GPU training enabled\")\n        \n        # Additional memory management for multi-GPU setup\n        torch.cuda.empty_cache()\n        # Set memory allocation strategy to reduce fragmentation\n        if hasattr(torch.cuda, 'memory_stats'):\n            print(\"Initial GPU memory allocated:\", torch.cuda.memory_allocated(0) / (1024**3), \"GB\")\nelse:\n    device = torch.device('cpu')\n    print(\"Using CPU - Note: Training will be much slower on CPU\")\n\n# Set random seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# %%\n# Dataset configuration - using the same dataset as the original notebook\nDATASET_ID = \"mvasiliniuc/iva-swift-codeint\"\n\n# Model configuration - using Phi-3-mini-128k-instruct\nMODEL_NAME = \"microsoft/Phi-3-mini-128k-instruct\"\nMAX_LENGTH = 4096  # Phi-3 can handle long sequences natively\nBATCH_SIZE = 2  # Reduced batch size for multi-GPU training (each GPU will process this batch size)\nLEARNING_RATE = 2e-5\nWEIGHT_DECAY = 0.01\nNUM_EPOCHS = 3\nWARMUP_RATIO = 0.03\nGRADIENT_ACCUMULATION_STEPS = 4  # Reduced since we're using 2 GPUs\n\n# LoRA configuration\nLORA_R = 16\nLORA_ALPHA = 32\nLORA_DROPOUT = 0.05\n\n# Debug mode for testing with smaller dataset\nDEBUG_MODE = False\nDEBUG_SAMPLE_SIZE = 100\n\nprint(f\"Using model: {MODEL_NAME}\")\nprint(f\"Max sequence length: {MAX_LENGTH}\")\nprint(f\"Batch size: {BATCH_SIZE} per device\")\nprint(f\"Effective batch size: {BATCH_SIZE * (2 if torch.cuda.device_count() > 1 else 1) * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"LoRA rank: {LORA_R}\")\n\n\n# %%\n# Function to load dataset with retry logic\ndef load_dataset_with_retry(dataset_id, max_retries=3, retry_delay=5):\n    \"\"\"Load a dataset with retry logic.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            print(f\"Loading dataset (attempt {attempt+1}/{max_retries})...\")\n            data = load_dataset(dataset_id, trust_remote_code=True)\n            print(f\"Dataset loaded successfully with {len(data['train'])} examples\")\n            return data\n        except Exception as e:\n            print(f\"Error loading dataset (attempt {attempt+1}/{max_retries}): {e}\")\n            if attempt < max_retries - 1:\n                print(f\"Retrying in {retry_delay} seconds...\")\n                time.sleep(retry_delay)\n            else:\n                print(\"Maximum retries reached. Could not load dataset.\")\n                raise\n\n# Load the dataset with retry logic\ntry:\n    print(f\"Loading dataset: {DATASET_ID}\")\n    data = load_dataset_with_retry(DATASET_ID)\n    print(\"Dataset structure:\")\n    print(data)\n    \n    # If in debug mode, take a small sample of the dataset\n    if DEBUG_MODE and 'train' in data:\n        print(f\"DEBUG MODE: Sampling {DEBUG_SAMPLE_SIZE} examples from dataset\")\n        # Take a stratified sample if possible\n        data['train'] = data['train'].shuffle(seed=42).select(range(min(DEBUG_SAMPLE_SIZE, len(data['train']))))\n        print(f\"Reduced dataset size: {len(data['train'])} examples\")\n        \nexcept Exception as e:\n    print(f\"Fatal error loading dataset: {e}\")\n    raise\n\n\n# %%\n# Verify dataset structure and column names\ndef verify_dataset_structure(dataset):\n    \"\"\"Verify that the dataset has the expected structure and columns.\"\"\"\n    required_columns = ['repo_name', 'path', 'content']\n    if 'train' not in dataset:\n        print(\"WARNING: Dataset does not have a 'train' split.\")\n        return False\n    \n    missing_columns = [col for col in required_columns if col not in dataset['train'].column_names]\n    if missing_columns:\n        print(f\"WARNING: Dataset is missing required columns: {missing_columns}\")\n        return False\n    \n    print(\"Dataset structure verification passed.\")\n    return True\n\n# Verify dataset structure\ndataset_valid = verify_dataset_structure(data)\nif not dataset_valid:\n    print(\"Dataset structure is not as expected. Proceeding with caution.\")\n\n# %%\n# Load the Phi-3 tokenizer\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=MAX_LENGTH)\n    # Add padding token if it doesn't exist\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n    print(f\"Tokenizer type: {tokenizer.__class__.__name__}\")\nexcept Exception as e:\n    print(f\"Error loading tokenizer: {e}\")\n    raise\n\n\n# %%\ndef extract_file_type(path):\n    \"\"\"\n    Extract the file type/category based on the file path and naming conventions in Swift projects.\n    \n    Args:\n        path (str): The file path\n        \n    Returns:\n        int: The category label (0-5)\n    \"\"\"\n    path_lower = path.lower()\n    filename = path.split('/')[-1].lower()\n    \n    # Category 0: Models - Data structures and model definitions\n    if ('model' in path_lower or \n        'struct' in path_lower or \n        'entity' in path_lower or\n        'data' in path_lower and 'class' in path_lower):\n        return 0\n    \n    # Category 1: Views - UI related files\n    elif ('view' in path_lower or \n          'ui' in path_lower or \n          'screen' in path_lower or \n          'page' in path_lower or\n          'controller' in path_lower and 'view' in path_lower):\n        return 1\n    \n    # Category 2: Controllers - Application logic\n    elif ('controller' in path_lower or \n          'manager' in path_lower or \n          'coordinator' in path_lower or\n          'service' in path_lower):\n        return 2\n    \n    # Category 3: Utilities - Helper functions and extensions\n    elif ('util' in path_lower or \n          'helper' in path_lower or \n          'extension' in path_lower or\n          'common' in path_lower):\n        return 3\n    \n    # Category 4: Tests - Test files\n    elif ('test' in path_lower or \n          'spec' in path_lower or \n          'mock' in path_lower):\n        return 4\n    \n    # Category 5: Configuration - Package and configuration files\n    elif ('package.swift' in path_lower or \n          'config' in path_lower or \n          'settings' in path_lower or\n          'info.plist' in path_lower):\n        return 5\n    \n    # Default to category 3 (Utilities) if no clear category is found\n    return 3\n\n# Define category names for better readability\ncategory_names = {\n    0: \"Models\",\n    1: \"Views\",\n    2: \"Controllers\",\n    3: \"Utilities\",\n    4: \"Tests\",\n    5: \"Configuration\"\n}\n\n# %%\n# Apply the function to create labels\ntry:\n    # Create a new column with the extracted labels\n    labeled_data = data['train'].map(lambda example: {\n        **example,\n        'label': extract_file_type(example['path'])\n    })\n    \n    # Count the distribution of labels\n    label_counts = collections.Counter(labeled_data['label'])\n    \n    print(\"Label distribution:\")\n    for label, count in sorted(label_counts.items()):\n        category_name = category_names.get(label, f\"Unknown-{label}\")\n        print(f\"Label {label} ({category_name}): {count} examples ({count/len(labeled_data)*100:.2f}%)\")\n    \n    # Get unique labels\n    unique_labels = sorted(label_counts.keys())\n    num_labels = len(unique_labels)\n    \n    print(f\"\\nTotal unique labels: {num_labels}\")\nexcept Exception as e:\n    print(f\"Error in data preparation: {e}\")\n    raise\n\n# %%\n# Split the data into train, validation, and test sets\ntry:\n    # Shuffle the data\n    shuffled_data = labeled_data.shuffle(seed=42)\n    \n    # Split into train (80%), validation (10%), and test (10%)\n    train_size = int(0.8 * len(shuffled_data))\n    val_size = int(0.1 * len(shuffled_data))\n    \n    train_data = shuffled_data.select(range(train_size))\n    val_data = shuffled_data.select(range(train_size, train_size + val_size))\n    test_data = shuffled_data.select(range(train_size + val_size, len(shuffled_data)))\n    \n    print(f\"Training set size: {len(train_data)}\")\n    print(f\"Training set label distribution: {collections.Counter(train_data['label'])}\")\n    print(f\"Validation set size: {len(val_data)}\")\n    print(f\"Validation set label distribution: {collections.Counter(val_data['label'])}\")\n    print(f\"Test set size: {len(test_data)}\")\n    print(f\"Test set label distribution: {collections.Counter(test_data['label'])}\")\nexcept Exception as e:\n    print(f\"Error splitting data: {e}\")\n    raise\n\n\n# %%\n# Create instruction-based prompts for the model\ndef create_instruction_prompt(example):\n    \"\"\"Convert a code example into an instruction-based prompt for language learning.\"\"\"\n    code = example['content']\n    label = example['label']\n    category = category_names.get(label, f\"Unknown-{label}\")\n    \n    # Create different types of prompts to help the model learn the language\n    prompt_types = [\n        # Explain code functionality\n        \"Explain what this Swift code does and how it works:\\n\\n\",\n        \n        # Identify patterns and features\n        \"Identify and explain the key Swift language features used in this code:\\n\\n\",\n        \n        # Complete or extend code\n        \"Complete or extend this Swift code with appropriate functionality:\\n\\n\",\n        \n        # Fix or improve code\n        \"Suggest improvements or best practices for this Swift code:\\n\\n\",\n        \n        # Understand code structure\n        f\"This is a Swift {category.lower()} file. Explain its structure and purpose:\\n\\n\",\n        \n        # Code generation tasks\n        \"Write a Swift function that accomplishes the same task as this code but more efficiently:\\n\\n\",\n        \n        # Language understanding\n        \"Explain the Swift syntax and language features demonstrated in this code:\\n\\n\",\n        \n        # Learning from examples\n        \"Study this Swift code example and explain what you can learn from it:\\n\\n\"\n    ]\n    \n    # Select a random prompt type\n    instruction = random.choice(prompt_types)\n    \n    code_section = f\"```swift\\n{code}\\n```\\n\\n\"\n    \n    # Create the full prompt\n    prompt = instruction + code_section\n    \n    # Create a detailed response based on the prompt type and code category\n    if \"Explain what this Swift code does\" in instruction:\n        response = f\"This Swift code is a {category.lower()} file that \"\n        if category == \"Models\":\n            response += \"defines data structures and model objects. \"\n        elif category == \"Views\":\n            response += \"implements user interface components. \"\n        elif category == \"Controllers\":\n            response += \"manages application logic and coordinates between models and views. \"\n        elif category == \"Utilities\":\n            response += \"provides helper functions and extensions. \"\n        elif category == \"Tests\":\n            response += \"contains test cases to verify functionality. \"\n        elif category == \"Configuration\":\n            response += \"configures application settings and parameters. \"\n        \n        response += \"The code uses Swift syntax with \"\n        \n        # Add some language-specific details based on code content\n        if \"class\" in code:\n            response += \"class definitions, \"\n        if \"struct\" in code:\n            response += \"struct definitions, \"\n        if \"func\" in code:\n            response += \"function declarations, \"\n        if \"var\" in code:\n            response += \"variable declarations, \"\n        if \"let\" in code:\n            response += \"constant declarations, \"\n        if \"guard\" in code or \"if let\" in code:\n            response += \"optional unwrapping, \"\n        if \"extension\" in code:\n            response += \"extensions, \"\n        if \"protocol\" in code:\n            response += \"protocol implementations, \"\n            \n        # Remove trailing comma and space if present\n        if response.endswith(\", \"):\n            response = response[:-2] + \".\"\n        else:\n            response += \"various Swift features.\"\n    \n    elif \"Identify and explain the key Swift language features\" in instruction:\n        response = \"This Swift code demonstrates several key language features:\\n\\n\"\n        \n        # Add language features based on code content\n        features = []\n        if \"class\" in code:\n            features.append(\"1. **Classes**: Swift classes are reference types that support inheritance and reference counting.\")\n        if \"struct\" in code:\n            features.append(\"1. **Structs**: Swift structs are value types that are copied when assigned or passed as arguments.\")\n        if \"protocol\" in code:\n            features.append(\"1. **Protocols**: Similar to interfaces in other languages, protocols define a blueprint of methods, properties, and requirements.\")\n        if \"extension\" in code:\n            features.append(\"1. **Extensions**: Swift allows adding functionality to existing types through extensions.\")\n        if \"guard\" in code:\n            features.append(\"1. **Guard statements**: Used for early returns and unwrapping optionals, improving code readability.\")\n        if \"if let\" in code or \"guard let\" in code:\n            features.append(\"1. **Optional binding**: Swift's way of safely unwrapping optional values.\")\n        if \"enum\" in code:\n            features.append(\"1. **Enumerations**: Swift enums are first-class types that can have methods and computed properties.\")\n        if \"func\" in code:\n            features.append(\"1. **Functions**: Swift functions can have parameters, return values, and support closures.\")\n        \n        # If no specific features were identified, add a generic response\n        if not features:\n            features.append(\"1. **Swift syntax**: The code demonstrates standard Swift syntax and conventions.\")\n            features.append(\"2. **Type safety**: Swift's strong type system helps prevent errors at compile time.\")\n            features.append(\"3. **Readability**: Swift's clean syntax makes code easy to read and maintain.\")\n        \n        # Renumber the features\n        for i, feature in enumerate(features):\n            feature_parts = feature.split(\": \", 1)\n            if len(feature_parts) == 2:\n                features[i] = f\"{i+1}. **{feature_parts[0].split('**')[1]}**: {feature_parts[1]}\"\n        \n        response += \"\\n\".join(features)\n    \n    elif \"Complete or extend this Swift code\" in instruction or \"Write a Swift function\" in instruction:\n        # For code generation tasks, provide a thoughtful response about how to approach the task\n        response = f\"To extend this Swift {category.lower()} code, I would consider the following approach:\\n\\n\"\n        \n        if category == \"Models\":\n            response += \"1. Add additional properties to capture more data attributes\\n\"\n            response += \"2. Implement Codable protocol for easy JSON serialization\\n\"\n            response += \"3. Add validation methods to ensure data integrity\\n\"\n            response += \"4. Include computed properties for derived values\\n\\n\"\n            response += \"Here's an implementation example:\\n\\n```swift\\n\"\n            \n            if \"struct\" in code:\n                response += \"// Extension to add Codable conformance\\nextension MyStruct: Codable {\\n    // Codable implementation\\n}\\n\\n\"\n                response += \"// Add validation method\\nextension MyStruct {\\n    func validate() -> Bool {\\n        // Validation logic\\n        return true\\n    }\\n}\\n\"\n            else:\n                response += \"// Example extension or additional functionality\\n// that would be appropriate for this model\\n\"\n            \n            response += \"```\"\n            \n        elif category == \"Views\":\n            response += \"1. Add UI customization options\\n\"\n            response += \"2. Implement additional user interaction handlers\\n\"\n            response += \"3. Add accessibility support\\n\"\n            response += \"4. Implement view lifecycle methods\\n\\n\"\n            response += \"Here's an implementation example:\\n\\n```swift\\n\"\n            response += \"// Example extension or additional functionality\\n// that would be appropriate for this view\\n\"\n            response += \"```\"\n            \n        else:\n            response += \"1. Add error handling to make the code more robust\\n\"\n            response += \"2. Implement additional helper methods\\n\"\n            response += \"3. Add documentation comments to improve code readability\\n\"\n            response += \"4. Consider performance optimizations where appropriate\\n\\n\"\n            response += \"Here's an implementation example:\\n\\n```swift\\n\"\n            response += \"// Example extension or additional functionality\\n// that would be appropriate for this code\\n\"\n            response += \"```\"\n    \n    else:\n        # Generic response for other prompt types\n        response = f\"This Swift code demonstrates typical patterns used in {category.lower()} files. \"\n        response += \"It follows Swift language conventions and showcases proper syntax for defining \"\n        \n        if category == \"Models\":\n            response += \"data structures with properties and methods. Swift models typically use structs for value semantics or classes when reference semantics are needed. The code demonstrates Swift's strong typing system and property declarations.\"\n        elif category == \"Views\":\n            response += \"UI components with layout and interaction logic. Swift views often use UIKit or SwiftUI frameworks, with clear separation of UI elements and their behaviors. The code shows how Swift handles user interface components and event responses.\"\n        elif category == \"Controllers\":\n            response += \"application logic and coordination between components. Controllers in Swift manage the flow of data between models and views, implementing business logic and handling user interactions. The code demonstrates Swift's approach to application architecture.\"\n        elif category == \"Utilities\":\n            response += \"helper functions and extensions to enhance functionality. Swift utilities often leverage the language's powerful extension capabilities to add functionality to existing types. The code shows how Swift can be extended and customized through utility functions.\"\n        elif category == \"Tests\":\n            response += \"test cases with setup, execution, and verification steps. Swift tests typically use XCTest framework with arrange-act-assert pattern. The code demonstrates Swift's approach to unit testing and verification.\"\n        elif category == \"Configuration\":\n            response += \"application settings and configuration parameters. Swift configuration files often define constants, environment settings, and application parameters. The code shows how Swift handles application configuration and settings management.\"\n    \n    # Combine prompt and response for instruction tuning\n    full_text = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n{response}\\n\"\n    \n    return {\n        \"text\": full_text,\n        \"prompt\": prompt,\n        \"response\": response,\n        \"label\": label,\n        \"category\": category\n    }\n\n\n# %%\n# Apply the function to create instruction-based datasets\ntry:\n    # Create instruction datasets\n    train_instructions = train_data.map(create_instruction_prompt)\n    val_instructions = val_data.map(create_instruction_prompt)\n    test_instructions = test_data.map(create_instruction_prompt)\n    \n    # Print an example to verify\n    print(\"Example instruction prompt:\")\n    print(\"-\" * 80)\n    print(train_instructions[0]['text'])\n    print(\"-\" * 80)\n    \n    print(f\"Created {len(train_instructions)} training instructions\")\n    print(f\"Created {len(val_instructions)} validation instructions\")\n    print(f\"Created {len(test_instructions)} test instructions\")\nexcept Exception as e:\n    print(f\"Error creating instruction prompts: {e}\")\n    raise\n\n\n# %%\n# FIXED: Tokenize the instruction data with proper handling of padding and truncation\ndef tokenize_instruction(examples):\n    \"\"\"Tokenize the instruction text with explicit padding and truncation settings.\"\"\"\n    # Process one example at a time to avoid dimension issues\n    results = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n    \n    for text in examples['text']:\n        # Tokenize with explicit padding and truncation settings\n        encoded = tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=MAX_LENGTH,\n            return_tensors=None  # Return Python lists, not PyTorch tensors\n        )\n        \n        # Add to results\n        results[\"input_ids\"].append(encoded[\"input_ids\"])\n        results[\"attention_mask\"].append(encoded[\"attention_mask\"])\n        results[\"labels\"].append(encoded[\"input_ids\"].copy())  # Copy input_ids for labels\n    \n    return results\n\n\n# %%\ntry:\n    # Apply tokenization to each split\n    tokenized_train = train_instructions.map(\n        tokenize_instruction,\n        batched=True,\n        remove_columns=['repo_name', 'path', 'content', 'label', 'text', 'prompt', 'response', 'category']\n    )\n    \n    tokenized_val = val_instructions.map(\n        tokenize_instruction,\n        batched=True,\n        remove_columns=['repo_name', 'path', 'content', 'label', 'text', 'prompt', 'response', 'category']\n    )\n    \n    # Set the format for PyTorch\n    tokenized_train.set_format(\"torch\")\n    tokenized_val.set_format(\"torch\")\n    \n    print(f\"Tokenized {len(tokenized_train)} training examples\")\n    print(f\"Tokenized {len(tokenized_val)} validation examples\")\n    print(\"Data tokenization complete\")\nexcept Exception as e:\n    print(f\"Error tokenizing data: {e}\")\n    raise\n\n# %%\n# Set up training arguments with optimized settings for multi-GPU training\ntry:\n    # Create output directory if it doesn't exist\n    os.makedirs(\"./phi3_swift_model\", exist_ok=True)\n    \n    # Configure training arguments with distributed training settings\n    training_args = TrainingArguments(\n        output_dir=\"./phi3_swift_model\",\n        num_train_epochs=NUM_EPOCHS,\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n        learning_rate=LEARNING_RATE,\n        weight_decay=WEIGHT_DECAY,\n        warmup_ratio=WARMUP_RATIO,\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        save_steps=500,\n        save_total_limit=2,\n        eval_strategy=\"steps\",\n        eval_steps=500,\n        load_best_model_at_end=True,\n        fp16=True,  # Use mixed precision training\n        gradient_checkpointing=True,  # Enable gradient checkpointing to save memory\n        # Distributed training parameters\n        local_rank=int(os.environ.get(\"LOCAL_RANK\", -1)),  # For distributed training\n        ddp_find_unused_parameters=False,  # Optimize DDP\n        dataloader_num_workers=4,  # Parallelize data loading\n        report_to=\"none\",  # Disable reporting to avoid extra overhead\n    )\n    \n    print(f\"Training arguments configured for {'multi-GPU' if torch.cuda.device_count() > 1 else 'single-GPU'} training\")\n    print(f\"Using gradient checkpointing: {training_args.gradient_checkpointing}\")\n    print(f\"Using mixed precision: {training_args.fp16}\")\n    print(f\"Local rank: {training_args.local_rank}\")\n    \nexcept Exception as e:\n    print(f\"Error setting up training arguments: {e}\")\n    raise\n\n# %%\n# Define early stopping callback\nearly_stopping_callback = EarlyStoppingCallback(\n    early_stopping_patience=3,\n    early_stopping_threshold=0.01\n)\n\n\n# %%\n# FIXED: Create a custom data collator that properly handles the data\nclass CustomDataCollatorForLanguageModeling(DataCollatorForLanguageModeling):\n    def __call__(self, features):\n        # Ensure all features have the same keys\n        if not all(k in features[0] for k in [\"input_ids\", \"attention_mask\", \"labels\"]):\n            raise ValueError(\"Some features are missing required keys\")\n        \n        # Create a batch with proper padding\n        batch = {\n            \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n            \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n            \"labels\": torch.stack([f[\"labels\"] for f in features])\n        }\n        \n        return batch\n\n# Create data collator for language modeling\ndata_collator = CustomDataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # We're doing causal language modeling, not masked language modeling\n)\n\n# %%\n# Create a flag to track which quantization method we're using\nUSING_AQLM = False\nQUANT_BITS = 2  # Default to 2-bit quantization\n\nprint(f\"Loading {MODEL_NAME} with {QUANT_BITS}-bit quantization...\")\n\ntry:\n    # First check if AQLM is available\n    if 'aqlm' in globals() or 'aqlm' in locals():\n        # Use AQLM's correct approach for 2-bit quantization\n        print(f\"Using AQLM for {QUANT_BITS}-bit quantization...\")\n        \n        # First load the model normally - we'll apply AQLM quantization after\n        print(f\"Loading base model {MODEL_NAME}...\")\n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_NAME,\n            torch_dtype=torch.float16,\n            device_map=\"auto\" if torch.cuda.is_available() else None,\n            trust_remote_code=True,\n            use_cache=False,  # Disable KV cache during training for better memory efficiency\n            low_cpu_mem_usage=True,  # Reduce CPU memory usage during loading\n        )\n        \n        # Now apply AQLM 2-bit quantization to the model\n        print(f\"Applying AQLM {QUANT_BITS}-bit quantization...\")\n        \n        try:\n            # Get the AQLM quantizer - try different possible module paths\n            try:\n                # Try different module paths where quantize function might be\n                if hasattr(aqlm, 'quantize'):\n                    quantize_fn = aqlm.quantize\n                elif hasattr(aqlm, 'quantization') and hasattr(aqlm.quantization, 'quantize'):\n                    quantize_fn = aqlm.quantization.quantize\n                else:\n                    # Try to discover the correct module\n                    for module_name in dir(aqlm):\n                        module = getattr(aqlm, module_name)\n                        if hasattr(module, 'quantize'):\n                            quantize_fn = module.quantize\n                            print(f\"Found quantize function in aqlm.{module_name}\")\n                            break\n                    else:\n                        raise ImportError(\"Could not find quantize function in AQLM modules\")\n                \n                # Apply quantization to the model\n                model = quantize_fn(\n                    model, \n                    bits=QUANT_BITS, \n                    lora_rank=LORA_R,  # Use the same rank as we'll use for LoRA\n                )\n                USING_AQLM = True\n                print(f\"Successfully applied AQLM {QUANT_BITS}-bit quantization\")\n            \n            except (ImportError, AttributeError) as e:\n                print(f\"Error using AQLM quantization: {e}\")\n                print(\"Trying alternative AQLM API...\")\n                \n                # Try an alternative approach with explicit package imports\n                from aqlm import quantize\n                model = quantize(\n                    model,\n                    bits=QUANT_BITS,\n                    lora_rank=LORA_R\n                )\n                USING_AQLM = True\n                print(f\"Successfully applied AQLM {QUANT_BITS}-bit quantization using direct import\")\n                \n        except Exception as quant_error:\n            print(f\"AQLM {QUANT_BITS}-bit quantization failed: {quant_error}\")\n            \n            # If 2-bit failed, try 4-bit with AQLM before falling back to BitsAndBytes\n            if QUANT_BITS == 2:\n                print(\"Trying AQLM with 4-bit quantization instead...\")\n                QUANT_BITS = 4\n                try:\n                    from aqlm import quantize\n                    model = quantize(\n                        model,\n                        bits=QUANT_BITS,\n                        lora_rank=LORA_R\n                    )\n                    USING_AQLM = True\n                    print(f\"Successfully applied AQLM {QUANT_BITS}-bit quantization\")\n                except Exception as e:\n                    print(f\"AQLM {QUANT_BITS}-bit quantization also failed: {e}\")\n                    raise  # Let it fall through to BitsAndBytes fallback\n            else:\n                raise  # Let it fall through to BitsAndBytes fallback\n    else:\n        raise ImportError(\"AQLM not available\")\n        \nexcept Exception as e:\n    # Fallback to using BitsAndBytes for 4-bit quantization\n    print(f\"Falling back to BitsAndBytes 4-bit quantization: {e}\")\n    QUANT_BITS = 4\n    USING_AQLM = False\n    \n    # Configure BitsAndBytes for 4-bit quantization\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True\n    )\n    \n    # Load model with BitsAndBytes 4-bit quantization\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        quantization_config=bnb_config,\n        device_map=\"auto\" if torch.cuda.is_available() else None,\n        torch_dtype=torch.float16,\n        trust_remote_code=True,\n        use_cache=False\n    )\n    print(\"Successfully loaded model with BitsAndBytes 4-bit quantization\")\n\n# Configure LoRA for fine-tuning (same for both quantization methods)\nprint(\"Setting up LoRA fine-tuning...\")\nlora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n)\n\n# Prepare the model for training with LoRA\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\n\n# Print information about the quantized model\nquant_method = \"AQLM\" if USING_AQLM else \"BitsAndBytes\"\nprint(f\"Model loaded and configured with {QUANT_BITS}-bit {quant_method} quantization and LoRA (rank={LORA_R})\")\nprint(f\"Model architecture: {model.__class__.__name__}\")\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M\")\n\n# %%\n# Create trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    callbacks=[early_stopping_callback]\n)\n\nprint(\"Training setup complete\")\n\n\n# %%\n# Function to monitor system resources during training\ndef monitor_resources():\n    process = psutil.Process(os.getpid())\n    memory_info = process.memory_info()\n    mem = psutil.virtual_memory()\n    cpu_percent = psutil.cpu_percent(interval=0.1)\n    \n    print(f\"\\nSystem Resources:\")\n    print(f\"CPU Usage: {cpu_percent}%\")\n    print(f\"Process Memory: {memory_info.rss / 1024 / 1024:.2f} MB\")\n    print(f\"System Memory: {mem.percent}% used, {mem.available / 1024 / 1024:.2f} MB available\\n\")\n\n\n# %%\n# Run training with enhanced memory monitoring for multi-GPU setup\ntry:\n    print(\"Starting training...\")\n    \n    # Monitor resources before training\n    print(\"Resources before training:\")\n    monitor_resources()\n    \n    # Additional memory cleanup before training\n    cleanup_memory()\n    \n    # Set PyTorch to optimize for multi-GPU training\n    if torch.cuda.device_count() > 1:\n        print(\"Configuring PyTorch for multi-GPU training...\")\n        # Enable TF32 precision for faster training (on Ampere GPUs)\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n        # Set memory allocation strategy\n        torch.cuda.set_per_process_memory_fraction(0.95)  # Reserve some memory for system\n    \n    # Start training with a timeout\n    start_time = time.time()\n    \n    # Run training\n    train_result = trainer.train()\n    \n    # Monitor resources after training\n    print(\"Resources after training:\")\n    monitor_resources()\n    \n    # Print training results\n    print(f\"Training completed in {train_result.metrics['train_runtime']:.2f} seconds\")\n    print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n    \n    # Save the model with appropriate method based on quantization used\n    trainer.save_model(\"./phi3_swift_model\")\n    print(f\"Model saved to ./phi3_swift_model ({QUANT_BITS}-bit {quant_method} quantized)\")\n    \n    # Save model configuration details\n    with open(\"./phi3_swift_model/quantization_config.json\", \"w\") as f:\n        config_data = {\n            \"quantization_method\": quant_method,\n            \"bits\": QUANT_BITS,\n            \"lora_rank\": LORA_R,\n            \"lora_alpha\": LORA_ALPHA,\n            \"original_model\": MODEL_NAME,\n            \"max_length\": MAX_LENGTH\n        }\n        json.dump(config_data, f, indent=2)\n    \n    # Create appropriate loading instructions based on quantization method\n    if USING_AQLM:\n        loading_code = \"\"\"```python\nfrom aqlm import quantize\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"./phi3_swift_model\")\n\n# Load the base model first (to apply quantization)\nbase_model = AutoModelForCausalLM.from_pretrained(\"./phi3_swift_model\")\n\n# Apply AQLM quantization\nmodel = quantize(base_model, bits=QUANT_BITS, lora_rank=LORA_R)\n```\"\"\"\n    else:\n        loading_code = \"\"\"```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"./phi3_swift_model\")\n\n# Configure 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True\n)\n\n# Load the quantized model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"./phi3_swift_model\",\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n```\"\"\"\n        \n    # Also save a README with information about the quantization\n    with open(\"./phi3_swift_model/README.md\", \"w\") as f:\n        f.write(f\"\"\"# Phi-3-mini Quantized Model\n\nThis model is a {QUANT_BITS}-bit quantized version of `{MODEL_NAME}` trained for Swift programming.\n\n## Quantization Details\n- Method: {quant_method}\n- Bits: {QUANT_BITS} \n- Training dataset: {DATASET_ID}\n- Fine-tuning method: LoRA (Low-Rank Adaptation)\n- LoRA rank: {LORA_R}\n- LoRA alpha: {LORA_ALPHA}\n\n## Usage\n\nTo load this model:\n\n{loading_code}\n\nThis quantized model reduces memory usage significantly while maintaining most of the capabilities of the original model.\n\"\"\")\n    \n    # Clean up memory\n    cleanup_memory()\n    \nexcept Exception as e:\n    print(f\"Error during training: {e}\")\n    \n    # Print stack trace for debugging\n    import traceback\n    traceback.print_exc()\n    \n    # Monitor resources after error\n    print(\"Resources after error:\")\n    monitor_resources()\n    \n    raise\n\n# %%\n# Test the model with Swift code examples\ntry:\n    print(f\"Testing the {QUANT_BITS}-bit {quant_method} quantized model with Swift code examples...\")\n    \n    # For testing, we use the model we already have loaded\n    test_model = model\n    \n    # Function to generate responses for test examples\n    def generate_response(prompt):\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            # Generate with the quantized model\n            outputs = test_model.generate(\n                inputs.input_ids,\n                max_new_tokens=200,\n                temperature=0.7,\n                top_p=0.9,\n                do_sample=True,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id\n            )\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        # Extract just the assistant's response\n        if \"<|assistant|>\" in response:\n            response = response.split(\"<|assistant|>\")[-1].strip()\n        return response\n    \n    # Test prompts for different Swift language tasks\n    test_prompts = [\n        # Explain Swift syntax\n        \"<|user|>\\nExplain the key features of Swift's optional unwrapping syntax:\\n\\n```swift\\nfunc processName(_ name: String?) {\\n    guard let unwrappedName = name else {\\n        print(\\\"No name provided\\\")\\n        return\\n    }\\n    print(\\\"Hello, \\\\(unwrappedName)!\\\")\\n}\\n```\\n<|assistant|>\",\n        \n        # Code completion\n        \"<|user|>\\nComplete this Swift function that calculates the factorial of a number:\\n\\n```swift\\nfunc factorial(_ n: Int) -> Int {\\n    // Add implementation here\\n}\\n```\\n<|assistant|>\",\n        \n        # Debugging help\n        \"<|user|>\\nWhat's wrong with this Swift code and how can I fix it?\\n\\n```swift\\nclass Person {\\n    var name: String\\n    var age: Int\\n    \\n    func greet() {\\n        print(\\\"Hello, my name is \\\\(name) and I am \\\\(age) years old.\\\")\\n    }\\n}\\n\\nlet person = Person()\\nperson.greet()\\n```\\n<|assistant|>\",\n        \n        # Swift best practices\n        \"<|user|>\\nExplain Swift best practices for error handling:\\n<|assistant|>\"\n    ]\n    \n    # Generate and print responses\n    for i, prompt in enumerate(test_prompts):\n        print(f\"\\nTest {i+1}:\\n{'-'*40}\")\n        print(f\"Prompt: {prompt.split('<|assistant|>')[0].replace('<|user|>', '')}\")\n        response = generate_response(prompt)\n        print(f\"\\nResponse:\\n{response}\\n\")\n    \n    print(\"\\nTesting complete\")\nexcept Exception as e:\n    print(f\"Error during testing: {e}\")\n    import traceback\n    traceback.print_exc()\n",
      "line_count": 1041,
      "code_type": "class",
      "code_name": "definitions",
      "file_size_bytes": 41589,
      "extraction_timestamp": "2025-05-11T21:41:20.745353"
    },
    {
      "language": "Python",
      "file_path": "Training/Train-Phi3-4bit.py",
      "content": "# ---\n# jupyter:\n#   jupytext:\n#     text_representation:\n#       extension: .py\n#       format_name: percent\n#       format_version: '1.3'\n#       jupytext_version: 1.17.1\n#   kernelspec:\n#     display_name: Python 3 (ipykernel)\n#     language: python\n#     name: python3\n# ---\n\n# %% [markdown]\n# # Training Phi-3-mini-128k-instruct to Learn Swift Programming Language\n#\n# This notebook trains Microsoft's Phi-3-mini-128k-instruct model to understand and work with Swift code using a dataset of real Swift files.\n\n# %%\n# Install required libraries\n!pip install transformers datasets evaluate torch scikit-learn tqdm dropbox requests accelerate peft bitsandbytes\n# Set PyTorch memory management environment variables to avoid fragmentation\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Explicitly set to use 2 GPUs\n\n# %%\n# Import required libraries\nimport torch\nimport numpy as np\nimport random\nimport time\nimport collections\nimport psutil\nimport os\nimport gc\nimport json\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    TrainingArguments, \n    Trainer, \n    DataCollatorForLanguageModeling,\n    BitsAndBytesConfig\n)\nfrom transformers.trainer_callback import EarlyStoppingCallback\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# Define memory cleanup function\ndef cleanup_memory():\n    \"\"\"Clean up GPU memory to avoid fragmentation.\"\"\"\n    print(\"Cleaning up memory...\")\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n        \n# Define resource monitoring function\ndef monitor_resources():\n    \"\"\"Monitor system and GPU resources.\"\"\"\n    # Monitor CPU and RAM\n    process = psutil.Process(os.getpid())\n    memory_info = process.memory_info()\n    print(f\"CPU memory usage: {memory_info.rss / 1024 / 1024:.2f} MB\")\n    \n    # Monitor GPU if available\n    if torch.cuda.is_available():\n        num_gpus = torch.cuda.device_count()\n        print(f\"Number of GPUs: {num_gpus}\")\n        \n        for i in range(num_gpus):\n            if hasattr(torch.cuda, 'memory_allocated'):\n                print(f\"GPU {i} ({torch.cuda.get_device_name(i)})\")\n                print(f\"  Memory allocated: {torch.cuda.memory_allocated(i) / (1024**3):.2f} GB\")\n                print(f\"  Memory reserved: {torch.cuda.memory_reserved(i) / (1024**3):.2f} GB\")\n                if hasattr(torch.cuda, 'memory_stats'):\n                    stats = torch.cuda.memory_stats(i)\n                    if 'active_bytes.all.current' in stats:\n                        print(f\"  Active memory: {stats['active_bytes.all.current'] / (1024**3):.2f} GB\")\n                    if 'reserved_bytes.all.current' in stats:\n                        print(f\"  Reserved memory: {stats['reserved_bytes.all.current'] / (1024**3):.2f} GB\")\n\n\n# %%\n# Check if GPU is available and configure for multi-GPU training\nif torch.cuda.is_available():\n    # Set up for distributed training on multiple GPUs\n    device = torch.device('cuda')\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n    \n    # Enable multi-GPU support for T4 x2\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs\")\n        # For distributed training, we'll use device_map=\"auto\" when loading the model\n        print(\"Multi-GPU training enabled\")\n        \n        # Additional memory management for multi-GPU setup\n        torch.cuda.empty_cache()\n        # Set memory allocation strategy to reduce fragmentation\n        if hasattr(torch.cuda, 'memory_stats'):\n            print(\"Initial GPU memory allocated:\", torch.cuda.memory_allocated(0) / (1024**3), \"GB\")\nelse:\n    device = torch.device('cpu')\n    print(\"Using CPU - Note: Training will be much slower on CPU\")\n\n# Set random seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# %%\n# Dataset configuration - using the same dataset as the original notebook\nDATASET_ID = \"mvasiliniuc/iva-swift-codeint\"\n\n# Model configuration - using Phi-3-mini-128k-instruct\nMODEL_NAME = \"microsoft/Phi-3-mini-128k-instruct\"\nMAX_LENGTH = 4096  # Phi-3 can handle long sequences natively\nBATCH_SIZE = 2  # Reduced batch size for multi-GPU training (each GPU will process this batch size)\nLEARNING_RATE = 2e-5\nWEIGHT_DECAY = 0.01\nNUM_EPOCHS = 3\nWARMUP_RATIO = 0.03\nGRADIENT_ACCUMULATION_STEPS = 4  # Reduced since we're using 2 GPUs\n\n# LoRA configuration\nLORA_R = 16\nLORA_ALPHA = 32\nLORA_DROPOUT = 0.05\n\n# Debug mode for testing with smaller dataset\nDEBUG_MODE = False\nDEBUG_SAMPLE_SIZE = 100\n\nprint(f\"Using model: {MODEL_NAME}\")\nprint(f\"Max sequence length: {MAX_LENGTH}\")\nprint(f\"Batch size: {BATCH_SIZE} per device\")\nprint(f\"Effective batch size: {BATCH_SIZE * (2 if torch.cuda.device_count() > 1 else 1) * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"LoRA rank: {LORA_R}\")\n\n\n# %%\n# Function to load dataset with retry logic\ndef load_dataset_with_retry(dataset_id, max_retries=3, retry_delay=5):\n    \"\"\"Load a dataset with retry logic.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            print(f\"Loading dataset (attempt {attempt+1}/{max_retries})...\")\n            data = load_dataset(dataset_id, trust_remote_code=True)\n            print(f\"Dataset loaded successfully with {len(data['train'])} examples\")\n            return data\n        except Exception as e:\n            print(f\"Error loading dataset (attempt {attempt+1}/{max_retries}): {e}\")\n            if attempt < max_retries - 1:\n                print(f\"Retrying in {retry_delay} seconds...\")\n                time.sleep(retry_delay)\n            else:\n                print(\"Maximum retries reached. Could not load dataset.\")\n                raise\n\n# Load the dataset with retry logic\ntry:\n    print(f\"Loading dataset: {DATASET_ID}\")\n    data = load_dataset_with_retry(DATASET_ID)\n    print(\"Dataset structure:\")\n    print(data)\n    \n    # If in debug mode, take a small sample of the dataset\n    if DEBUG_MODE and 'train' in data:\n        print(f\"DEBUG MODE: Sampling {DEBUG_SAMPLE_SIZE} examples from dataset\")\n        # Take a stratified sample if possible\n        data['train'] = data['train'].shuffle(seed=42).select(range(min(DEBUG_SAMPLE_SIZE, len(data['train']))))\n        print(f\"Reduced dataset size: {len(data['train'])} examples\")\n        \nexcept Exception as e:\n    print(f\"Fatal error loading dataset: {e}\")\n    raise\n\n\n# %%\n# Verify dataset structure and column names\ndef verify_dataset_structure(dataset):\n    \"\"\"Verify that the dataset has the expected structure and columns.\"\"\"\n    required_columns = ['repo_name', 'path', 'content']\n    if 'train' not in dataset:\n        print(\"WARNING: Dataset does not have a 'train' split.\")\n        return False\n    \n    missing_columns = [col for col in required_columns if col not in dataset['train'].column_names]\n    if missing_columns:\n        print(f\"WARNING: Dataset is missing required columns: {missing_columns}\")\n        return False\n    \n    print(\"Dataset structure verification passed.\")\n    return True\n\n# Verify dataset structure\ndataset_valid = verify_dataset_structure(data)\nif not dataset_valid:\n    print(\"Dataset structure is not as expected. Proceeding with caution.\")\n\n# %%\n# Load the Phi-3 tokenizer\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=MAX_LENGTH)\n    # Add padding token if it doesn't exist\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n    print(f\"Tokenizer type: {tokenizer.__class__.__name__}\")\nexcept Exception as e:\n    print(f\"Error loading tokenizer: {e}\")\n    raise\n\n\n# %%\ndef extract_file_type(path):\n    \"\"\"\n    Extract the file type/category based on the file path and naming conventions in Swift projects.\n    \n    Args:\n        path (str): The file path\n        \n    Returns:\n        int: The category label (0-5)\n    \"\"\"\n    path_lower = path.lower()\n    filename = path.split('/')[-1].lower()\n    \n    # Category 0: Models - Data structures and model definitions\n    if ('model' in path_lower or \n        'struct' in path_lower or \n        'entity' in path_lower or\n        'data' in path_lower and 'class' in path_lower):\n        return 0\n    \n    # Category 1: Views - UI related files\n    elif ('view' in path_lower or \n          'ui' in path_lower or \n          'screen' in path_lower or \n          'page' in path_lower or\n          'controller' in path_lower and 'view' in path_lower):\n        return 1\n    \n    # Category 2: Controllers - Application logic\n    elif ('controller' in path_lower or \n          'manager' in path_lower or \n          'coordinator' in path_lower or\n          'service' in path_lower):\n        return 2\n    \n    # Category 3: Utilities - Helper functions and extensions\n    elif ('util' in path_lower or \n          'helper' in path_lower or \n          'extension' in path_lower or\n          'common' in path_lower):\n        return 3\n    \n    # Category 4: Tests - Test files\n    elif ('test' in path_lower or \n          'spec' in path_lower or \n          'mock' in path_lower):\n        return 4\n    \n    # Category 5: Configuration - Package and configuration files\n    elif ('package.swift' in path_lower or \n          'config' in path_lower or \n          'settings' in path_lower or\n          'info.plist' in path_lower):\n        return 5\n    \n    # Default to category 3 (Utilities) if no clear category is found\n    return 3\n\n# Define category names for better readability\ncategory_names = {\n    0: \"Models\",\n    1: \"Views\",\n    2: \"Controllers\",\n    3: \"Utilities\",\n    4: \"Tests\",\n    5: \"Configuration\"\n}\n\n# %%\n# Apply the function to create labels\ntry:\n    # Create a new column with the extracted labels\n    labeled_data = data['train'].map(lambda example: {\n        **example,\n        'label': extract_file_type(example['path'])\n    })\n    \n    # Count the distribution of labels\n    label_counts = collections.Counter(labeled_data['label'])\n    \n    print(\"Label distribution:\")\n    for label, count in sorted(label_counts.items()):\n        category_name = category_names.get(label, f\"Unknown-{label}\")\n        print(f\"Label {label} ({category_name}): {count} examples ({count/len(labeled_data)*100:.2f}%)\")\n    \n    # Get unique labels\n    unique_labels = sorted(label_counts.keys())\n    num_labels = len(unique_labels)\n    \n    print(f\"\\nTotal unique labels: {num_labels}\")\nexcept Exception as e:\n    print(f\"Error in data preparation: {e}\")\n    raise\n\n# %%\n# Split the data into train, validation, and test sets\ntry:\n    # Shuffle the data\n    shuffled_data = labeled_data.shuffle(seed=42)\n    \n    # Split into train (80%), validation (10%), and test (10%)\n    train_size = int(0.8 * len(shuffled_data))\n    val_size = int(0.1 * len(shuffled_data))\n    \n    train_data = shuffled_data.select(range(train_size))\n    val_data = shuffled_data.select(range(train_size, train_size + val_size))\n    test_data = shuffled_data.select(range(train_size + val_size, len(shuffled_data)))\n    \n    print(f\"Training set size: {len(train_data)}\")\n    print(f\"Training set label distribution: {collections.Counter(train_data['label'])}\")\n    print(f\"Validation set size: {len(val_data)}\")\n    print(f\"Validation set label distribution: {collections.Counter(val_data['label'])}\")\n    print(f\"Test set size: {len(test_data)}\")\n    print(f\"Test set label distribution: {collections.Counter(test_data['label'])}\")\nexcept Exception as e:\n    print(f\"Error splitting data: {e}\")\n    raise\n\n\n# %%\n# Create instruction-based prompts for the model\ndef create_instruction_prompt(example):\n    \"\"\"Convert a code example into an instruction-based prompt for language learning.\"\"\"\n    code = example['content']\n    label = example['label']\n    category = category_names.get(label, f\"Unknown-{label}\")\n    \n    # Create different types of prompts to help the model learn the language\n    prompt_types = [\n        # Explain code functionality\n        \"Explain what this Swift code does and how it works:\\n\\n\",\n        \n        # Identify patterns and features\n        \"Identify and explain the key Swift language features used in this code:\\n\\n\",\n        \n        # Complete or extend code\n        \"Complete or extend this Swift code with appropriate functionality:\\n\\n\",\n        \n        # Fix or improve code\n        \"Suggest improvements or best practices for this Swift code:\\n\\n\",\n        \n        # Understand code structure\n        f\"This is a Swift {category.lower()} file. Explain its structure and purpose:\\n\\n\",\n        \n        # Code generation tasks\n        \"Write a Swift function that accomplishes the same task as this code but more efficiently:\\n\\n\",\n        \n        # Language understanding\n        \"Explain the Swift syntax and language features demonstrated in this code:\\n\\n\",\n        \n        # Learning from examples\n        \"Study this Swift code example and explain what you can learn from it:\\n\\n\"\n    ]\n    \n    # Select a random prompt type\n    instruction = random.choice(prompt_types)\n    \n    code_section = f\"```swift\\n{code}\\n```\\n\\n\"\n    \n    # Create the full prompt\n    prompt = instruction + code_section\n    \n    # Create a detailed response based on the prompt type and code category\n    if \"Explain what this Swift code does\" in instruction:\n        response = f\"This Swift code is a {category.lower()} file that \"\n        if category == \"Models\":\n            response += \"defines data structures and model objects. \"\n        elif category == \"Views\":\n            response += \"implements user interface components. \"\n        elif category == \"Controllers\":\n            response += \"manages application logic and coordinates between models and views. \"\n        elif category == \"Utilities\":\n            response += \"provides helper functions and extensions. \"\n        elif category == \"Tests\":\n            response += \"contains test cases to verify functionality. \"\n        elif category == \"Configuration\":\n            response += \"configures application settings and parameters. \"\n        \n        response += \"The code uses Swift syntax with \"\n        \n        # Add some language-specific details based on code content\n        if \"class\" in code:\n            response += \"class definitions, \"\n        if \"struct\" in code:\n            response += \"struct definitions, \"\n        if \"func\" in code:\n            response += \"function declarations, \"\n        if \"var\" in code:\n            response += \"variable declarations, \"\n        if \"let\" in code:\n            response += \"constant declarations, \"\n        if \"guard\" in code or \"if let\" in code:\n            response += \"optional unwrapping, \"\n        if \"extension\" in code:\n            response += \"extensions, \"\n        if \"protocol\" in code:\n            response += \"protocol implementations, \"\n            \n        # Remove trailing comma and space if present\n        if response.endswith(\", \"):\n            response = response[:-2] + \".\"\n        else:\n            response += \"various Swift features.\"\n    \n    elif \"Identify and explain the key Swift language features\" in instruction:\n        response = \"This Swift code demonstrates several key language features:\\n\\n\"\n        \n        # Add language features based on code content\n        features = []\n        if \"class\" in code:\n            features.append(\"1. **Classes**: Swift classes are reference types that support inheritance and reference counting.\")\n        if \"struct\" in code:\n            features.append(\"1. **Structs**: Swift structs are value types that are copied when assigned or passed as arguments.\")\n        if \"protocol\" in code:\n            features.append(\"1. **Protocols**: Similar to interfaces in other languages, protocols define a blueprint of methods, properties, and requirements.\")\n        if \"extension\" in code:\n            features.append(\"1. **Extensions**: Swift allows adding functionality to existing types through extensions.\")\n        if \"guard\" in code:\n            features.append(\"1. **Guard statements**: Used for early returns and unwrapping optionals, improving code readability.\")\n        if \"if let\" in code or \"guard let\" in code:\n            features.append(\"1. **Optional binding**: Swift's way of safely unwrapping optional values.\")\n        if \"enum\" in code:\n            features.append(\"1. **Enumerations**: Swift enums are first-class types that can have methods and computed properties.\")\n        if \"func\" in code:\n            features.append(\"1. **Functions**: Swift functions can have parameters, return values, and support closures.\")\n        \n        # If no specific features were identified, add a generic response\n        if not features:\n            features.append(\"1. **Swift syntax**: The code demonstrates standard Swift syntax and conventions.\")\n            features.append(\"2. **Type safety**: Swift's strong type system helps prevent errors at compile time.\")\n            features.append(\"3. **Readability**: Swift's clean syntax makes code easy to read and maintain.\")\n        \n        # Renumber the features\n        for i, feature in enumerate(features):\n            feature_parts = feature.split(\": \", 1)\n            if len(feature_parts) == 2:\n                features[i] = f\"{i+1}. **{feature_parts[0].split('**')[1]}**: {feature_parts[1]}\"\n        \n        response += \"\\n\".join(features)\n    \n    elif \"Complete or extend this Swift code\" in instruction or \"Write a Swift function\" in instruction:\n        # For code generation tasks, provide a thoughtful response about how to approach the task\n        response = f\"To extend this Swift {category.lower()} code, I would consider the following approach:\\n\\n\"\n        \n        if category == \"Models\":\n            response += \"1. Add additional properties to capture more data attributes\\n\"\n            response += \"2. Implement Codable protocol for easy JSON serialization\\n\"\n            response += \"3. Add validation methods to ensure data integrity\\n\"\n            response += \"4. Include computed properties for derived values\\n\\n\"\n            response += \"Here's an implementation example:\\n\\n```swift\\n\"\n            \n            if \"struct\" in code:\n                response += \"// Extension to add Codable conformance\\nextension MyStruct: Codable {\\n    // Codable implementation\\n}\\n\\n\"\n                response += \"// Add validation method\\nextension MyStruct {\\n    func validate() -> Bool {\\n        // Validation logic\\n        return true\\n    }\\n}\\n\"\n            else:\n                response += \"// Example extension or additional functionality\\n// that would be appropriate for this model\\n\"\n            \n            response += \"```\"\n            \n        elif category == \"Views\":\n            response += \"1. Add UI customization options\\n\"\n            response += \"2. Implement additional user interaction handlers\\n\"\n            response += \"3. Add accessibility support\\n\"\n            response += \"4. Implement view lifecycle methods\\n\\n\"\n            response += \"Here's an implementation example:\\n\\n```swift\\n\"\n            response += \"// Example extension or additional functionality\\n// that would be appropriate for this view\\n\"\n            response += \"```\"\n            \n        else:\n            response += \"1. Add error handling to make the code more robust\\n\"\n            response += \"2. Implement additional helper methods\\n\"\n            response += \"3. Add documentation comments to improve code readability\\n\"\n            response += \"4. Consider performance optimizations where appropriate\\n\\n\"\n            response += \"Here's an implementation example:\\n\\n```swift\\n\"\n            response += \"// Example extension or additional functionality\\n// that would be appropriate for this code\\n\"\n            response += \"```\"\n    \n    else:\n        # Generic response for other prompt types\n        response = f\"This Swift code demonstrates typical patterns used in {category.lower()} files. \"\n        response += \"It follows Swift language conventions and showcases proper syntax for defining \"\n        \n        if category == \"Models\":\n            response += \"data structures with properties and methods. Swift models typically use structs for value semantics or classes when reference semantics are needed. The code demonstrates Swift's strong typing system and property declarations.\"\n        elif category == \"Views\":\n            response += \"UI components with layout and interaction logic. Swift views often use UIKit or SwiftUI frameworks, with clear separation of UI elements and their behaviors. The code shows how Swift handles user interface components and event responses.\"\n        elif category == \"Controllers\":\n            response += \"application logic and coordination between components. Controllers in Swift manage the flow of data between models and views, implementing business logic and handling user interactions. The code demonstrates Swift's approach to application architecture.\"\n        elif category == \"Utilities\":\n            response += \"helper functions and extensions to enhance functionality. Swift utilities often leverage the language's powerful extension capabilities to add functionality to existing types. The code shows how Swift can be extended and customized through utility functions.\"\n        elif category == \"Tests\":\n            response += \"test cases with setup, execution, and verification steps. Swift tests typically use XCTest framework with arrange-act-assert pattern. The code demonstrates Swift's approach to unit testing and verification.\"\n        elif category == \"Configuration\":\n            response += \"application settings and configuration parameters. Swift configuration files often define constants, environment settings, and application parameters. The code shows how Swift handles application configuration and settings management.\"\n    \n    # Combine prompt and response for instruction tuning\n    full_text = f\"<|user|>\\n{prompt}\\n<|assistant|>\\n{response}\\n\"\n    \n    return {\n        \"text\": full_text,\n        \"prompt\": prompt,\n        \"response\": response,\n        \"label\": label,\n        \"category\": category\n    }\n\n\n# %%\n# Apply the function to create instruction-based datasets\ntry:\n    # Create instruction datasets\n    train_instructions = train_data.map(create_instruction_prompt)\n    val_instructions = val_data.map(create_instruction_prompt)\n    test_instructions = test_data.map(create_instruction_prompt)\n    \n    # Print an example to verify\n    print(\"Example instruction prompt:\")\n    print(\"-\" * 80)\n    print(train_instructions[0]['text'])\n    print(\"-\" * 80)\n    \n    print(f\"Created {len(train_instructions)} training instructions\")\n    print(f\"Created {len(val_instructions)} validation instructions\")\n    print(f\"Created {len(test_instructions)} test instructions\")\nexcept Exception as e:\n    print(f\"Error creating instruction prompts: {e}\")\n    raise\n\n\n# %%\n# FIXED: Tokenize the instruction data with proper handling of padding and truncation\ndef tokenize_instruction(examples):\n    \"\"\"Tokenize the instruction text with explicit padding and truncation settings.\"\"\"\n    # Process one example at a time to avoid dimension issues\n    results = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n    \n    for text in examples['text']:\n        # Tokenize with explicit padding and truncation settings\n        encoded = tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=MAX_LENGTH,\n            return_tensors=None  # Return Python lists, not PyTorch tensors\n        )\n        \n        # Add to results\n        results[\"input_ids\"].append(encoded[\"input_ids\"])\n        results[\"attention_mask\"].append(encoded[\"attention_mask\"])\n        results[\"labels\"].append(encoded[\"input_ids\"].copy())  # Copy input_ids for labels\n    \n    return results\n\n\n# %%\ntry:\n    # Apply tokenization to each split\n    tokenized_train = train_instructions.map(\n        tokenize_instruction,\n        batched=True,\n        remove_columns=['repo_name', 'path', 'content', 'label', 'text', 'prompt', 'response', 'category']\n    )\n    \n    tokenized_val = val_instructions.map(\n        tokenize_instruction,\n        batched=True,\n        remove_columns=['repo_name', 'path', 'content', 'label', 'text', 'prompt', 'response', 'category']\n    )\n    \n    # Set the format for PyTorch\n    tokenized_train.set_format(\"torch\")\n    tokenized_val.set_format(\"torch\")\n    \n    print(f\"Tokenized {len(tokenized_train)} training examples\")\n    print(f\"Tokenized {len(tokenized_val)} validation examples\")\n    print(\"Data tokenization complete\")\nexcept Exception as e:\n    print(f\"Error tokenizing data: {e}\")\n    raise\n\n# %%\n# Set up training arguments with optimized settings for multi-GPU training\ntry:\n    # Create output directory if it doesn't exist\n    os.makedirs(\"./phi3_swift_model\", exist_ok=True)\n    \n    # Configure training arguments with distributed training settings\n    training_args = TrainingArguments(\n        output_dir=\"./phi3_swift_model\",\n        num_train_epochs=NUM_EPOCHS,\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n        learning_rate=LEARNING_RATE,\n        weight_decay=WEIGHT_DECAY,\n        warmup_ratio=WARMUP_RATIO,\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        save_steps=500,\n        save_total_limit=2,\n        eval_strategy=\"steps\",\n        eval_steps=500,\n        load_best_model_at_end=True,\n        fp16=True,  # Use mixed precision training\n        gradient_checkpointing=True,  # Enable gradient checkpointing to save memory\n        # Distributed training parameters\n        local_rank=int(os.environ.get(\"LOCAL_RANK\", -1)),  # For distributed training\n        ddp_find_unused_parameters=False,  # Optimize DDP\n        dataloader_num_workers=4,  # Parallelize data loading\n        report_to=\"none\",  # Disable reporting to avoid extra overhead\n    )\n    \n    print(f\"Training arguments configured for {'multi-GPU' if torch.cuda.device_count() > 1 else 'single-GPU'} training\")\n    print(f\"Using gradient checkpointing: {training_args.gradient_checkpointing}\")\n    print(f\"Using mixed precision: {training_args.fp16}\")\n    print(f\"Local rank: {training_args.local_rank}\")\n    \nexcept Exception as e:\n    print(f\"Error setting up training arguments: {e}\")\n    raise\n\n# %%\n# Define early stopping callback\nearly_stopping_callback = EarlyStoppingCallback(\n    early_stopping_patience=3,\n    early_stopping_threshold=0.01\n)\n\n\n# %%\n# FIXED: Create a custom data collator that properly handles the data\nclass CustomDataCollatorForLanguageModeling(DataCollatorForLanguageModeling):\n    def __call__(self, features):\n        # Ensure all features have the same keys\n        if not all(k in features[0] for k in [\"input_ids\", \"attention_mask\", \"labels\"]):\n            raise ValueError(\"Some features are missing required keys\")\n        \n        # Create a batch with proper padding\n        batch = {\n            \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n            \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n            \"labels\": torch.stack([f[\"labels\"] for f in features])\n        }\n        \n        return batch\n\n# Create data collator for language modeling\ndata_collator = CustomDataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # We're doing causal language modeling, not masked language modeling\n)\n\n# %%\n# Create trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    callbacks=[early_stopping_callback]\n)\n\nprint(\"Training setup complete\")\n\n\n# %%\n# Function to monitor system resources during training\ndef monitor_resources():\n    process = psutil.Process(os.getpid())\n    memory_info = process.memory_info()\n    mem = psutil.virtual_memory()\n    cpu_percent = psutil.cpu_percent(interval=0.1)\n    \n    print(f\"\\nSystem Resources:\")\n    print(f\"CPU Usage: {cpu_percent}%\")\n    print(f\"Process Memory: {memory_info.rss / 1024 / 1024:.2f} MB\")\n    print(f\"System Memory: {mem.percent}% used, {mem.available / 1024 / 1024:.2f} MB available\\n\")\n\n\n# %%\n# Run training with enhanced memory monitoring for multi-GPU setup\ntry:\n    print(\"Starting training...\")\n    \n    # Monitor resources before training\n    print(\"Resources before training:\")\n    monitor_resources()\n    \n    # Additional memory cleanup before training\n    cleanup_memory()\n    \n    # Set PyTorch to optimize for multi-GPU training\n    if torch.cuda.device_count() > 1:\n        print(\"Configuring PyTorch for multi-GPU training...\")\n        # Enable TF32 precision for faster training (on Ampere GPUs)\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n        # Set memory allocation strategy\n        torch.cuda.set_per_process_memory_fraction(0.95)  # Reserve some memory for system\n    \n    # Start training with a timeout\n    start_time = time.time()\n    \n    # Run training\n    train_result = trainer.train()\n    \n    # Monitor resources after training\n    print(\"Resources after training:\")\n    monitor_resources()\n    \n    # Print training results\n    print(f\"Training completed in {train_result.metrics['train_runtime']:.2f} seconds\")\n    print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n    \n    # Save the model\n    trainer.save_model(\"./phi3_swift_model\")\n    print(\"Model saved to ./phi3_swift_model\")\n    \n    # Clean up memory\n    cleanup_memory()\n    \nexcept Exception as e:\n    print(f\"Error during training: {e}\")\n    \n    # Print stack trace for debugging\n    import traceback\n    traceback.print_exc()\n    \n    # Monitor resources after error\n    print(\"Resources after error:\")\n    monitor_resources()\n    \n    raise\n\n# %%\n# Test the model with Swift code examples\ntry:\n    print(\"Testing the model with Swift code examples...\")\n    \n    # Function to generate responses for test examples\n    def generate_response(prompt):\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = model.generate(\n                inputs.input_ids,\n                max_new_tokens=200,\n                temperature=0.7,\n                top_p=0.9,\n                do_sample=True,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id\n            )\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        # Extract just the assistant's response\n        if \"<|assistant|>\" in response:\n            response = response.split(\"<|assistant|>\")[-1].strip()\n        return response\n    \n    # Test prompts for different Swift language tasks\n    test_prompts = [\n        # Explain Swift syntax\n        \"<|user|>\\nExplain the key features of Swift's optional unwrapping syntax:\\n\\n```swift\\nfunc processName(_ name: String?) {\\n    guard let unwrappedName = name else {\\n        print(\\\"No name provided\\\")\\n        return\\n    }\\n    print(\\\"Hello, \\\\(unwrappedName)!\\\")\\n}\\n```\\n<|assistant|>\",\n        \n        # Code completion\n        \"<|user|>\\nComplete this Swift function that calculates the factorial of a number:\\n\\n```swift\\nfunc factorial(_ n: Int) -> Int {\\n    // Add implementation here\\n}\\n```\\n<|assistant|>\",\n        \n        # Debugging help\n        \"<|user|>\\nWhat's wrong with this Swift code and how can I fix it?\\n\\n```swift\\nclass Person {\\n    var name: String\\n    var age: Int\\n    \\n    func greet() {\\n        print(\\\"Hello, my name is \\\\(name) and I am \\\\(age) years old.\\\")\\n    }\\n}\\n\\nlet person = Person()\\nperson.greet()\\n```\\n<|assistant|>\",\n        \n        # Swift best practices\n        \"<|user|>\\nExplain Swift best practices for error handling:\\n<|assistant|>\"\n    ]\n    \n    # Generate and print responses\n    for i, prompt in enumerate(test_prompts):\n        print(f\"\\nTest {i+1}:\\n{'-'*40}\")\n        print(f\"Prompt: {prompt.split('<|assistant|>')[0].replace('<|user|>', '')}\")\n        response = generate_response(prompt)\n        print(f\"\\nResponse:\\n{response}\\n\")\n    \n    print(\"\\nTesting complete\")\nexcept Exception as e:\n    print(f\"Error during testing: {e}\")\n    import traceback\n    traceback.print_exc()\n",
      "line_count": 808,
      "code_type": "class",
      "code_name": "definitions",
      "file_size_bytes": 32515,
      "extraction_timestamp": "2025-05-11T21:41:20.745600"
    },
    {
      "language": "Python",
      "file_path": "Scripts/extract_code_samples.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nGitHub Code Sample Extractor\n\nThis script traverses a GitHub repository, extracts code samples from various\nprogramming languages, and creates a structured dataset.\n\nUsage:\n    python extract_code_samples.py [--output OUTPUT_DIR] [--min-lines MIN_LINES] [--max-samples MAX_SAMPLES]\n\nThe script will:\n1. Scan the current repository (or provided path)\n2. Identify code files based on extensions\n3. Extract code samples with metadata\n4. Save the dataset in JSON format\n\"\"\"\n\nimport os\nimport json\nimport argparse\nimport random\nfrom pathlib import Path\nfrom collections import defaultdict\nimport datetime\nimport shutil\nimport re\n\n# Language detection based on file extensions\nLANGUAGE_EXTENSIONS = {\n    # Web development\n    'js': 'JavaScript',\n    'jsx': 'JavaScript (React)',\n    'ts': 'TypeScript',\n    'tsx': 'TypeScript (React)',\n    'html': 'HTML',\n    'css': 'CSS',\n    'scss': 'SCSS',\n    'less': 'LESS',\n    \n    # Mobile development\n    'swift': 'Swift',\n    'kt': 'Kotlin',\n    'java': 'Java',\n    'h': 'C/Objective-C Header',\n    'm': 'Objective-C',\n    'mm': 'Objective-C++',\n    \n    # General programming\n    'py': 'Python',\n    'rb': 'Ruby',\n    'php': 'PHP',\n    'go': 'Go',\n    'rs': 'Rust',\n    'c': 'C',\n    'cpp': 'C++',\n    'cc': 'C++',\n    'cxx': 'C++',\n    'cs': 'C#',\n    'fs': 'F#',\n    'pl': 'Perl',\n    'sh': 'Shell',\n    'bash': 'Bash',\n    'lua': 'Lua',\n    'r': 'R',\n    'dart': 'Dart',\n    'jl': 'Julia',\n    'ex': 'Elixir',\n    'exs': 'Elixir',\n    'elm': 'Elm',\n    'clj': 'Clojure',\n    'scala': 'Scala',\n    'hs': 'Haskell',\n    'erl': 'Erlang',\n    \n    # Configuration and data\n    'json': 'JSON',\n    'yaml': 'YAML',\n    'yml': 'YAML',\n    'toml': 'TOML',\n    'xml': 'XML',\n    'sql': 'SQL',\n    'graphql': 'GraphQL',\n    'proto': 'Protocol Buffer',\n    \n    # Game development\n    'gd': 'GDScript',\n    'cs': 'C# (Unity)',\n    'unity': 'Unity',\n    'unrealscript': 'UnrealScript',\n    \n    # Other\n    'md': 'Markdown',\n    'rst': 'reStructuredText',\n    'tex': 'LaTeX',\n    'gradle': 'Gradle',\n    'bat': 'Batch',\n    'ps1': 'PowerShell',\n    'vb': 'Visual Basic',\n    'asm': 'Assembly',\n    'vue': 'Vue',\n    'svelte': 'Svelte',\n}\n\n# Directories to ignore during traversal\nIGNORE_DIRS = {\n    '.git', '.github', 'node_modules', 'venv', '.venv', '.env', \n    'env', '__pycache__', 'build', 'dist', 'target', 'out',\n    '.idea', '.vscode', '.DS_Store', 'bin', 'obj', '.next',\n    'coverage', '.coverage', 'tmp', 'temp', 'log', 'logs'\n}\n\n# Files to ignore \nIGNORE_FILES = {\n    '.gitignore', '.gitattributes', '.gitmodules', '.npmrc', '.npmignore',\n    'package-lock.json', 'yarn.lock', 'Pipfile.lock', 'Gemfile.lock',\n    '.eslintrc', '.prettierrc', '.editorconfig', '.DS_Store', 'thumbs.db',\n    'LICENSE', 'LICENSE.md', 'LICENSE.txt', 'NOTICE', 'CONTRIBUTORS',\n    'CODEOWNERS', '.travis.yml', 'appveyor.yml', 'azure-pipelines.yml',\n    'Dockerfile', 'docker-compose.yml', '.dockerignore'\n}\n\ndef parse_arguments():\n    \"\"\"Parse command line arguments.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Extract code samples from a GitHub repository\")\n    parser.add_argument(\"--repo-path\", type=str, default=\".\",\n                        help=\"Path to the repository (default: current directory)\")\n    parser.add_argument(\"--output\", type=str, default=\"./dataset\",\n                        help=\"Output directory for the dataset (default: ./dataset)\")\n    parser.add_argument(\"--min-lines\", type=int, default=5,\n                        help=\"Minimum number of lines for a code sample (default: 5)\")\n    parser.add_argument(\"--max-samples\", type=int, default=1000,\n                        help=\"Maximum number of samples per language (default: 1000)\")\n    parser.add_argument(\"--include-dirs\", type=str, default=\"\",\n                        help=\"Comma-separated list of directories to include (default: all)\")\n    parser.add_argument(\"--include-langs\", type=str, default=\"\",\n                        help=\"Comma-separated list of languages to include (default: all)\")\n    parser.add_argument(\"--exclude-dirs\", type=str, default=\"\",\n                        help=\"Comma-separated list of additional directories to exclude\")\n    parser.add_argument(\"--exclude-langs\", type=str, default=\"\",\n                        help=\"Comma-separated list of languages to exclude\")\n    parser.add_argument(\"--seed\", type=int, default=None,\n                        help=\"Random seed for reproducibility\")\n    parser.add_argument(\"--export-format\", type=str, choices=[\"json\", \"csv\", \"jsonl\", \"all\"], \n                        default=\"json\", help=\"Export format for the dataset\")\n    return parser.parse_args()\n\ndef is_binary_file(file_path, sample_size=8192):\n    \"\"\"Check if a file is binary by reading a sample and looking for null bytes.\"\"\"\n    try:\n        with open(file_path, 'rb') as f:\n            sample = f.read(sample_size)\n            # If there's a null byte in the sample, it's likely binary\n            if b'\\x00' in sample:\n                return True\n            # Also check for non-text characters\n            try:\n                sample.decode('utf-8')\n                return False\n            except UnicodeDecodeError:\n                return True\n    except Exception:\n        # If there was an error reading the file, consider it binary to be safe\n        return True\n\ndef detect_language(file_path):\n    \"\"\"Detect programming language from file extension.\"\"\"\n    extension = file_path.split('.')[-1].lower() if '.' in file_path else \"\"\n    return LANGUAGE_EXTENSIONS.get(extension, \"Unknown\")\n\ndef should_process_file(file_path, args, include_langs, exclude_langs):\n    \"\"\"Determine if a file should be processed based on criteria.\"\"\"\n    # Skip files in IGNORE_FILES\n    if os.path.basename(file_path) in IGNORE_FILES:\n        return False\n    \n    # Skip binary files\n    if is_binary_file(file_path):\n        return False\n        \n    # Check if the language is in the include list or exclude list\n    language = detect_language(file_path)\n    if language == \"Unknown\":\n        return False\n        \n    if include_langs and language not in include_langs:\n        return False\n        \n    if language in exclude_langs:\n        return False\n    \n    return True\n\ndef extract_code_sample(file_path, min_lines=5):\n    \"\"\"Extract code from a file with metadata.\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n            content = f.read()\n        \n        lines = content.split('\\n')\n        if len(lines) < min_lines:\n            return None\n            \n        language = detect_language(file_path)\n        rel_path = os.path.relpath(file_path)\n        \n        # Try to extract class/function definition using regex\n        code_type = \"unknown\"\n        code_name = \"\"\n        \n        if language in [\"Python\", \"Ruby\", \"JavaScript\", \"TypeScript\"]:\n            # Look for function or class definitions\n            class_match = re.search(r'class\\s+(\\w+)', content)\n            func_match = re.search(r'(def|function)\\s+(\\w+)', content)\n            \n            if class_match:\n                code_type = \"class\"\n                code_name = class_match.group(1)\n            elif func_match:\n                code_type = \"function\"\n                code_name = func_match.group(2)\n        \n        return {\n            \"language\": language,\n            \"file_path\": rel_path,\n            \"content\": content,\n            \"line_count\": len(lines),\n            \"code_type\": code_type,\n            \"code_name\": code_name,\n            \"file_size_bytes\": os.path.getsize(file_path),\n            \"extraction_timestamp\": datetime.datetime.now().isoformat()\n        }\n    except Exception as e:\n        print(f\"Error processing {file_path}: {e}\")\n        return None\n\ndef traverse_repository(args):\n    \"\"\"Traverse the repository and extract code samples.\"\"\"\n    repo_path = Path(args.repo_path).resolve()\n    print(f\"Scanning repository at: {repo_path}\")\n    \n    # Process include/exclude lists\n    include_dirs = set(args.include_dirs.split(',')) if args.include_dirs else set()\n    exclude_dirs = set(IGNORE_DIRS)\n    if args.exclude_dirs:\n        exclude_dirs.update(args.exclude_dirs.split(','))\n    \n    include_langs = set(args.include_langs.split(',')) if args.include_langs else set()\n    exclude_langs = set(args.exclude_langs.split(',')) if args.exclude_langs else set()\n    \n    # Set random seed if provided\n    if args.seed is not None:\n        random.seed(args.seed)\n    \n    # Collect samples by language\n    samples_by_language = defaultdict(list)\n    \n    # Walk through the repository\n    for root, dirs, files in os.walk(repo_path):\n        # Skip excluded directories\n        dirs[:] = [d for d in dirs if d not in exclude_dirs]\n        \n        # If include_dirs is provided, only include those directories\n        if include_dirs and not any(d in root for d in include_dirs):\n            continue\n            \n        for file in files:\n            file_path = os.path.join(root, file)\n            \n            if should_process_file(file_path, args, include_langs, exclude_langs):\n                sample = extract_code_sample(file_path, args.min_lines)\n                if sample:\n                    language = sample[\"language\"]\n                    # Only add if we haven't reached the max samples for this language\n                    if len(samples_by_language[language]) < args.max_samples:\n                        samples_by_language[language].append(sample)\n    \n    # Create a flat list of all samples\n    all_samples = []\n    for language, samples in samples_by_language.items():\n        all_samples.extend(samples)\n    \n    # Shuffle the samples to ensure diversity\n    random.shuffle(all_samples)\n    \n    return all_samples, samples_by_language\n\ndef export_dataset(samples, samples_by_language, args):\n    \"\"\"Export the collected samples to a dataset.\"\"\"\n    output_dir = Path(args.output)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Create metadata about the dataset\n    metadata = {\n        \"dataset_name\": \"GitHub Code Samples\",\n        \"description\": \"A dataset of code samples extracted from a GitHub repository\",\n        \"creation_date\": datetime.datetime.now().isoformat(),\n        \"total_samples\": len(samples),\n        \"languages\": {lang: len(lang_samples) for lang, lang_samples in samples_by_language.items()},\n        \"extraction_parameters\": {\n            \"min_lines\": args.min_lines,\n            \"max_samples_per_language\": args.max_samples,\n            \"included_directories\": args.include_dirs.split(',') if args.include_dirs else \"all\",\n            \"excluded_directories\": list(IGNORE_DIRS) + (args.exclude_dirs.split(',') if args.exclude_dirs else []),\n            \"included_languages\": args.include_langs.split(',') if args.include_langs else \"all\",\n            \"excluded_languages\": args.exclude_langs.split(',') if args.exclude_langs else [],\n        }\n    }\n    \n    # Export in the requested format(s)\n    if args.export_format in [\"json\", \"all\"]:\n        with open(output_dir / \"code_samples_dataset.json\", 'w', encoding='utf-8') as f:\n            json.dump({\"metadata\": metadata, \"samples\": samples}, f, indent=2)\n        \n        # Also save a separate file for each language\n        lang_dir = output_dir / \"by_language\"\n        lang_dir.mkdir(exist_ok=True)\n        \n        for language, lang_samples in samples_by_language.items():\n            safe_lang_name = language.replace('/', '_').replace('#', 'Sharp').replace('+', 'Plus')\n            with open(lang_dir / f\"{safe_lang_name}.json\", 'w', encoding='utf-8') as f:\n                json.dump({\"language\": language, \"samples\": lang_samples}, f, indent=2)\n    \n    if args.export_format in [\"csv\", \"all\"]:\n        import csv\n        \n        # Create a flattened version of the data for CSV export\n        with open(output_dir / \"code_samples_dataset.csv\", 'w', newline='', encoding='utf-8') as f:\n            fieldnames = [\"language\", \"file_path\", \"line_count\", \"code_type\", \"code_name\", \"file_size_bytes\", \"extraction_timestamp\"]\n            writer = csv.DictWriter(f, fieldnames=fieldnames)\n            \n            writer.writeheader()\n            for sample in samples:\n                row = {field: sample[field] for field in fieldnames}\n                writer.writerow(row)\n    \n    if args.export_format in [\"jsonl\", \"all\"]:\n        with open(output_dir / \"code_samples_dataset.jsonl\", 'w', encoding='utf-8') as f:\n            for sample in samples:\n                f.write(json.dumps(sample) + '\\n')\n    \n    # Always save metadata separately\n    with open(output_dir / \"dataset_metadata.json\", 'w', encoding='utf-8') as f:\n        json.dump(metadata, f, indent=2)\n    \n    print(f\"\\nDataset exported to: {output_dir}\")\n    print(f\"Total samples: {len(samples)}\")\n    print(\"Samples by language:\")\n    for language, count in sorted(metadata[\"languages\"].items(), key=lambda x: x[1], reverse=True):\n        print(f\"  {language}: {count}\")\n\ndef main():\n    \"\"\"Main function to run the code extraction.\"\"\"\n    args = parse_arguments()\n    \n    print(\"Starting GitHub Code Sample Extractor\")\n    print(\"-------------------------------------\")\n    \n    # Traverse the repository and collect samples\n    print(\"\\nScanning repository for code samples...\")\n    samples, samples_by_language = traverse_repository(args)\n    \n    if not samples:\n        print(\"No code samples found matching the criteria.\")\n        return\n    \n    # Export the dataset\n    print(f\"\\nFound {len(samples)} code samples across {len(samples_by_language)} languages.\")\n    export_dataset(samples, samples_by_language, args)\n    \n    print(\"\\nExtraction complete!\")\n\nif __name__ == \"__main__\":\n    main()\n",
      "line_count": 374,
      "code_type": "class",
      "code_name": "definitions",
      "file_size_bytes": 13716,
      "extraction_timestamp": "2025-05-11T21:41:20.745828"
    },
    {
      "language": "Python",
      "file_path": "Scripts/analyze_code_dataset.py",
      "content": "#!/usr/bin/env python3\n\"\"\"\nGitHub Code Dataset Analyzer\n\nThis script loads and analyzes datasets created by the extract_code_samples.py script,\ngenerating visualizations and statistics about the code.\n\nUsage:\n    python analyze_code_dataset.py [--dataset DATASET_PATH] [--output OUTPUT_DIR]\n\nRequirements:\n    - matplotlib\n    - pandas\n    - seaborn (optional, for enhanced visualizations)\n\"\"\"\n\nimport os\nimport json\nimport argparse\nimport datetime\nfrom pathlib import Path\nimport csv\nimport re\nfrom collections import Counter, defaultdict\n\n# Try to import visualization libraries, but make them optional\ntry:\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    VISUALIZATION_AVAILABLE = True\n    \n    try:\n        import seaborn as sns\n        sns.set_style(\"whitegrid\")\n        SEABORN_AVAILABLE = True\n    except ImportError:\n        SEABORN_AVAILABLE = False\n        print(\"Seaborn not found. Basic visualizations will be used.\")\nexcept ImportError:\n    VISUALIZATION_AVAILABLE = False\n    print(\"Matplotlib and/or pandas not found. Visualizations will be disabled.\")\n    print(\"To enable visualizations, install required packages:\")\n    print(\"pip install matplotlib pandas seaborn\")\n\ndef parse_arguments():\n    \"\"\"Parse command line arguments.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Analyze code samples dataset\")\n    parser.add_argument(\"--dataset\", type=str, default=\"./dataset\",\n                        help=\"Path to the dataset directory (default: ./dataset)\")\n    parser.add_argument(\"--output\", type=str, default=\"./dataset/analysis\",\n                        help=\"Output directory for analysis results (default: ./dataset/analysis)\")\n    parser.add_argument(\"--format\", type=str, choices=[\"json\", \"csv\", \"jsonl\"], default=\"json\",\n                        help=\"Dataset format to analyze (default: json)\")\n    parser.add_argument(\"--no-vis\", action=\"store_true\",\n                        help=\"Disable visualizations even if dependencies are available\")\n    return parser.parse_args()\n\ndef load_dataset(args):\n    \"\"\"Load the dataset from the specified path and format.\"\"\"\n    dataset_path = Path(args.dataset)\n    \n    # Determine the dataset file path based on format\n    if args.format == \"json\":\n        dataset_file = dataset_path / \"code_samples_dataset.json\"\n    elif args.format == \"csv\":\n        dataset_file = dataset_path / \"code_samples_dataset.csv\"\n    elif args.format == \"jsonl\":\n        dataset_file = dataset_path / \"code_samples_dataset.jsonl\"\n    \n    # Check if the dataset file exists\n    if not dataset_file.exists():\n        print(f\"Error: Dataset file not found at {dataset_file}\")\n        return None, None\n    \n    # Load the dataset\n    samples = []\n    metadata = None\n    \n    try:\n        if args.format == \"json\":\n            with open(dataset_file, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                samples = data.get(\"samples\", [])\n                metadata = data.get(\"metadata\", {})\n                \n            # Try to load metadata from separate file if not found in the main file\n            if not metadata:\n                metadata_file = dataset_path / \"dataset_metadata.json\"\n                if metadata_file.exists():\n                    with open(metadata_file, 'r', encoding='utf-8') as f:\n                        metadata = json.load(f)\n                        \n        elif args.format == \"csv\":\n            with open(dataset_file, 'r', encoding='utf-8', newline='') as f:\n                reader = csv.DictReader(f)\n                samples = list(reader)\n                \n            # Try to load metadata from separate file\n            metadata_file = dataset_path / \"dataset_metadata.json\"\n            if metadata_file.exists():\n                with open(metadata_file, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                    \n        elif args.format == \"jsonl\":\n            with open(dataset_file, 'r', encoding='utf-8') as f:\n                samples = [json.loads(line) for line in f if line.strip()]\n                \n            # Try to load metadata from separate file\n            metadata_file = dataset_path / \"dataset_metadata.json\"\n            if metadata_file.exists():\n                with open(metadata_file, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n    \n    except Exception as e:\n        print(f\"Error loading dataset: {e}\")\n        return None, None\n        \n    return samples, metadata\n\ndef convert_to_dataframe(samples):\n    \"\"\"Convert samples to a pandas DataFrame if pandas is available.\"\"\"\n    if not VISUALIZATION_AVAILABLE:\n        return None\n        \n    # For CSV datasets, samples might already have the correct structure\n    # For JSON/JSONL, we need to extract the content and flatten the structure\n    processed_samples = []\n    \n    for sample in samples:\n        # Check if 'content' is in the sample, if not it's probably already processed\n        if 'content' in sample:\n            # Create a copy without the content field (to avoid huge DataFrame)\n            processed_sample = {k: v for k, v in sample.items() if k != 'content'}\n            \n            # Calculate additional metrics\n            content = sample['content']\n            lines = content.split('\\n')\n            \n            # Calculate complexity metrics\n            processed_sample['char_count'] = len(content)\n            processed_sample['line_count'] = len(lines)\n            processed_sample['avg_line_length'] = len(content) / max(len(lines), 1)\n            \n            # Calculate comment metrics for common languages\n            comment_count = 0\n            if sample['language'] in ['Python', 'Ruby', 'Shell', 'Bash']:\n                comment_count = sum(1 for line in lines if line.strip().startswith('#'))\n            elif sample['language'] in ['JavaScript', 'TypeScript', 'Java', 'C', 'C++', 'C#']:\n                comment_count = sum(1 for line in lines if line.strip().startswith('//'))\n                # Also count /* */ comments\n                comment_count += content.count('/*')\n            \n            processed_sample['comment_count'] = comment_count\n            processed_sample['comment_ratio'] = comment_count / max(len(lines), 1)\n            \n            processed_samples.append(processed_sample)\n        else:\n            # Convert numeric fields if they're stored as strings (common in CSV)\n            for field in ['line_count', 'file_size_bytes']:\n                if field in sample and isinstance(sample[field], str):\n                    try:\n                        sample[field] = int(sample[field])\n                    except (ValueError, TypeError):\n                        pass\n                        \n            processed_samples.append(sample)\n    \n    # Convert to DataFrame\n    df = pd.DataFrame(processed_samples)\n    \n    # Convert timestamps to datetime if present\n    if 'extraction_timestamp' in df.columns:\n        try:\n            df['extraction_timestamp'] = pd.to_datetime(df['extraction_timestamp'])\n        except Exception:\n            pass\n            \n    return df\n\ndef generate_basic_statistics(samples, metadata=None):\n    \"\"\"Generate basic statistics from the dataset.\"\"\"\n    if not samples:\n        return {}\n        \n    # Count samples by language\n    languages = Counter(sample.get('language', 'Unknown') for sample in samples)\n    \n    # Calculate average file size and line count\n    avg_file_size = sum(int(sample.get('file_size_bytes', 0)) for sample in samples) / len(samples)\n    avg_line_count = sum(int(sample.get('line_count', 0)) for sample in samples) / len(samples)\n    \n    # Count code types\n    code_types = Counter(sample.get('code_type', 'unknown') for sample in samples)\n    \n    # Calculate statistics by language\n    lang_stats = defaultdict(lambda: {\n        'count': 0, \n        'avg_size': 0, \n        'avg_lines': 0, \n        'file_paths': []\n    })\n    \n    for sample in samples:\n        lang = sample.get('language', 'Unknown')\n        lang_stats[lang]['count'] += 1\n        lang_stats[lang]['avg_size'] += int(sample.get('file_size_bytes', 0))\n        lang_stats[lang]['avg_lines'] += int(sample.get('line_count', 0))\n        lang_stats[lang]['file_paths'].append(sample.get('file_path', ''))\n    \n    # Calculate averages\n    for lang, stats in lang_stats.items():\n        stats['avg_size'] /= stats['count']\n        stats['avg_lines'] /= stats['count']\n        # Limit the number of file paths to avoid massive output\n        stats['file_paths'] = stats['file_paths'][:5]\n    \n    # Top languages by sample count\n    top_languages = languages.most_common(10)\n    \n    # Largest files\n    largest_files = sorted(\n        [\n            (sample.get('file_path', ''), \n             int(sample.get('file_size_bytes', 0)),\n             sample.get('language', 'Unknown'))\n        for sample in samples\n        ],\n        key=lambda x: x[1],\n        reverse=True\n    )[:10]\n    \n    # Most complex files (by line count)\n    most_complex_files = sorted(\n        [\n            (sample.get('file_path', ''), \n             int(sample.get('line_count', 0)),\n             sample.get('language', 'Unknown'))\n        for sample in samples\n        ],\n        key=lambda x: x[1],\n        reverse=True\n    )[:10]\n    \n    # Combine all statistics\n    stats = {\n        'total_samples': len(samples),\n        'languages': dict(languages),\n        'top_languages': dict(top_languages),\n        'avg_file_size_bytes': avg_file_size,\n        'avg_line_count': avg_line_count,\n        'code_types': dict(code_types),\n        'language_stats': {k: v for k, v in lang_stats.items()},\n        'largest_files': largest_files,\n        'most_complex_files': most_complex_files,\n        'analysis_timestamp': datetime.datetime.now().isoformat()\n    }\n    \n    # Add metadata if available\n    if metadata:\n        stats['original_metadata'] = metadata\n    \n    return stats\n\ndef create_visualizations(df, output_dir):\n    \"\"\"Create visualizations from the data.\"\"\"\n    if not VISUALIZATION_AVAILABLE or df is None or df.empty:\n        return False\n        \n    os.makedirs(output_dir, exist_ok=True)\n    \n    # 1. Language Distribution (Bar Chart)\n    plt.figure(figsize=(12, 8))\n    if SEABORN_AVAILABLE:\n        ax = sns.countplot(y='language', data=df, order=df['language'].value_counts().index[:15])\n        ax.set_title('Top 15 Programming Languages in the Repository', fontsize=16)\n    else:\n        language_counts = df['language'].value_counts().head(15)\n        language_counts.plot(kind='barh')\n        plt.title('Top 15 Programming Languages in the Repository', fontsize=16)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(output_dir, 'language_distribution.png'))\n    plt.close()\n    \n    # 2. File Size Distribution\n    plt.figure(figsize=(10, 6))\n    if 'file_size_bytes' in df.columns:\n        if SEABORN_AVAILABLE:\n            sns.histplot(data=df, x='file_size_bytes', bins=50, log_scale=True)\n        else:\n            plt.hist(df['file_size_bytes'], bins=50, log=True)\n        plt.title('File Size Distribution (log scale)', fontsize=16)\n        plt.xlabel('File Size (bytes)')\n        plt.ylabel('Count')\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, 'file_size_distribution.png'))\n    plt.close()\n    \n    # 3. Line Count Distribution\n    plt.figure(figsize=(10, 6))\n    if 'line_count' in df.columns:\n        if SEABORN_AVAILABLE:\n            sns.histplot(data=df, x='line_count', bins=50)\n        else:\n            plt.hist(df['line_count'], bins=50)\n        plt.title('Line Count Distribution', fontsize=16)\n        plt.xlabel('Number of Lines')\n        plt.ylabel('Count')\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, 'line_count_distribution.png'))\n    plt.close()\n    \n    # 4. Average File Size by Language\n    plt.figure(figsize=(12, 8))\n    if 'file_size_bytes' in df.columns and 'language' in df.columns:\n        avg_size_by_lang = df.groupby('language')['file_size_bytes'].mean().sort_values(ascending=False).head(15)\n        avg_size_by_lang.plot(kind='barh')\n        plt.title('Average File Size by Language (Top 15)', fontsize=16)\n        plt.xlabel('Average Size (bytes)')\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, 'avg_size_by_language.png'))\n    plt.close()\n    \n    # 5. Average Line Count by Language\n    plt.figure(figsize=(12, 8))\n    if 'line_count' in df.columns and 'language' in df.columns:\n        avg_lines_by_lang = df.groupby('language')['line_count'].mean().sort_values(ascending=False).head(15)\n        avg_lines_by_lang.plot(kind='barh')\n        plt.title('Average Line Count by Language (Top 15)', fontsize=16)\n        plt.xlabel('Average Line Count')\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, 'avg_lines_by_language.png'))\n    plt.close()\n    \n    # 6. Code Type Distribution\n    plt.figure(figsize=(10, 6))\n    if 'code_type' in df.columns:\n        if SEABORN_AVAILABLE:\n            sns.countplot(x='code_type', data=df)\n        else:\n            df['code_type'].value_counts().plot(kind='bar')\n        plt.title('Code Type Distribution', fontsize=16)\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, 'code_type_distribution.png'))\n    plt.close()\n    \n    # 7. Comment Ratio by Language (if available)\n    if 'comment_ratio' in df.columns and 'language' in df.columns:\n        plt.figure(figsize=(12, 8))\n        avg_comment_ratio = df.groupby('language')['comment_ratio'].mean().sort_values(ascending=False).head(15)\n        avg_comment_ratio.plot(kind='barh')\n        plt.title('Average Comment Ratio by Language (Top 15)', fontsize=16)\n        plt.xlabel('Average Comment Ratio (comments per line)')\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, 'comment_ratio_by_language.png'))\n        plt.close()\n    \n    return True\n\ndef generate_html_report(stats, visualization_created, output_dir):\n    \"\"\"Generate an HTML report of the analysis results.\"\"\"\n    output_path = os.path.join(output_dir, 'code_analysis_report.html')\n    \n    html = f\"\"\"\n    <!DOCTYPE html>\n    <html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n        <title>Code Repository Analysis Report</title>\n        <style>\n            body {{\n                font-family: Arial, sans-serif;\n                line-height: 1.6;\n                color: #333;\n                max-width: 1200px;\n                margin: 0 auto;\n                padding: 20px;\n            }}\n            h1, h2, h3 {{\n                color: #0066cc;\n            }}\n            table {{\n                border-collapse: collapse;\n                width: 100%;\n                margin-bottom: 20px;\n            }}\n            th, td {{\n                border: 1px solid #ddd;\n                padding: 8px;\n                text-align: left;\n            }}\n            th {{\n                background-color: #f2f2f2;\n            }}\n            tr:nth-child(even) {{\n                background-color: #f9f9f9;\n            }}\n            .visualization {{\n                max-width: 100%;\n                margin: 20px 0;\n                box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n            }}\n            .stats-container {{\n                display: flex;\n                flex-wrap: wrap;\n                justify-content: space-between;\n            }}\n            .stat-box {{\n                background-color: #f8f9fa;\n                border-radius: 8px;\n                padding: 15px;\n                margin-bottom: 20px;\n                box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n                width: 23%;\n            }}\n            .stat-box h3 {{\n                margin-top: 0;\n                border-bottom: 1px solid #ddd;\n                padding-bottom: 5px;\n            }}\n            @media (max-width: 768px) {{\n                .stat-box {{\n                    width: 48%;\n                }}\n            }}\n            @media (max-width: 480px) {{\n                .stat-box {{\n                    width: 100%;\n                }}\n            }}\n        </style>\n    </head>\n    <body>\n        <h1>Code Repository Analysis Report</h1>\n        <p>Analysis performed on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n        \n        <h2>Overview</h2>\n        <div class=\"stats-container\">\n            <div class=\"stat-box\">\n                <h3>Total Samples</h3>\n                <p>{stats.get('total_samples', 0)}</p>\n            </div>\n            <div class=\"stat-box\">\n                <h3>Languages</h3>\n                <p>{len(stats.get('languages', {}))}</p>\n            </div>\n            <div class=\"stat-box\">\n                <h3>Avg File Size</h3>\n                <p>{stats.get('avg_file_size_bytes', 0):.2f} bytes</p>\n            </div>\n            <div class=\"stat-box\">\n                <h3>Avg Line Count</h3>\n                <p>{stats.get('avg_line_count', 0):.2f}</p>\n            </div>\n        </div>\n    \"\"\"\n    \n    # Add visualizations if created\n    if visualization_created:\n        html += \"\"\"\n        <h2>Visualizations</h2>\n        <div class=\"visualizations-container\">\n            <h3>Language Distribution</h3>\n            <img class=\"visualization\" src=\"language_distribution.png\" alt=\"Language Distribution\">\n            \n            <h3>File Size Distribution</h3>\n            <img class=\"visualization\" src=\"file_size_distribution.png\" alt=\"File Size Distribution\">\n            \n            <h3>Line Count Distribution</h3>\n            <img class=\"visualization\" src=\"line_count_distribution.png\" alt=\"Line Count Distribution\">\n            \n            <h3>Average File Size by Language</h3>\n            <img class=\"visualization\" src=\"avg_size_by_language.png\" alt=\"Average File Size by Language\">\n            \n            <h3>Average Line Count by Language</h3>\n            <img class=\"visualization\" src=\"avg_lines_by_language.png\" alt=\"Average Line Count by Language\">\n            \n            <h3>Code Type Distribution</h3>\n            <img class=\"visualization\" src=\"code_type_distribution.png\" alt=\"Code Type Distribution\">\n        \"\"\"\n        \n        # Add comment ratio visualization if available\n        comment_ratio_img = os.path.join(output_dir, 'comment_ratio_by_language.png')\n        if os.path.exists(comment_ratio_img):\n            html += \"\"\"\n            <h3>Comment Ratio by Language</h3>\n            <img class=\"visualization\" src=\"comment_ratio_by_language.png\" alt=\"Comment Ratio by Language\">\n            \"\"\"\n            \n        html += \"</div>\"\n    \n    # Top languages\n    html += \"\"\"\n        <h2>Top Languages</h2>\n        <table>\n            <tr>\n                <th>Language</th>\n                <th>Sample Count</th>\n                <th>Percentage</th>\n            </tr>\n    \"\"\"\n    \n    total_samples = stats.get('total_samples', 0)\n    for lang, count in stats.get('top_languages', {}).items():\n        percentage = (count / total_samples) * 100 if total_samples > 0 else 0\n        html += f\"\"\"\n            <tr>\n                <td>{lang}</td>\n                <td>{count}</td>\n                <td>{percentage:.2f}%</td>\n            </tr>\n        \"\"\"\n    \n    html += \"</table>\"\n    \n    # Largest files\n    html += \"\"\"\n        <h2>Largest Files</h2>\n        <table>\n            <tr>\n                <th>File Path</th>\n                <th>Size (bytes)</th>\n                <th>Language</th>\n            </tr>\n    \"\"\"\n    \n    for file_path, size, lang in stats.get('largest_files', []):\n        html += f\"\"\"\n            <tr>\n                <td>{file_path}</td>\n                <td>{size}</td>\n                <td>{lang}</td>\n            </tr>\n        \"\"\"\n    \n    html += \"</table>\"\n    \n    # Most complex files\n    html += \"\"\"\n        <h2>Most Complex Files (by Line Count)</h2>\n        <table>\n            <tr>\n                <th>File Path</th>\n                <th>Line Count</th>\n                <th>Language</th>\n            </tr>\n    \"\"\"\n    \n    for file_path, line_count, lang in stats.get('most_complex_files', []):\n        html += f\"\"\"\n            <tr>\n                <td>{file_path}</td>\n                <td>{line_count}</td>\n                <td>{lang}</td>\n            </tr>\n        \"\"\"\n    \n    html += \"</table>\"\n    \n    # Language statistics\n    html += \"\"\"\n        <h2>Language Statistics</h2>\n        <table>\n            <tr>\n                <th>Language</th>\n                <th>Count</th>\n                <th>Avg Size (bytes)</th>\n                <th>Avg Lines</th>\n                <th>Sample File Paths</th>\n            </tr>\n    \"\"\"\n    \n    # Sort languages by count\n    sorted_langs = sorted(\n        stats.get('language_stats', {}).items(),\n        key=lambda x: x[1]['count'],\n        reverse=True\n    )\n    \n    for lang, lang_stats in sorted_langs:\n        file_paths = \", \".join(lang_stats.get('file_paths', []))\n        html += f\"\"\"\n            <tr>\n                <td>{lang}</td>\n                <td>{lang_stats.get('count', 0)}</td>\n                <td>{lang_stats.get('avg_size', 0):.2f}</td>\n                <td>{lang_stats.get('avg_lines', 0):.2f}</td>\n                <td>{file_paths}</td>\n            </tr>\n        \"\"\"\n    \n    html += \"</table>\"\n    \n    # Code type distribution\n    html += \"\"\"\n        <h2>Code Type Distribution</h2>\n        <table>\n            <tr>\n                <th>Code Type</th>\n                <th>Count</th>\n                <th>Percentage</th>\n            </tr>\n    \"\"\"\n    \n    code_types = stats.get('code_types', {})\n    for code_type, count in code_types.items():\n        percentage = (count / total_samples) * 100 if total_samples > 0 else 0\n        html += f\"\"\"\n            <tr>\n                <td>{code_type}</td>\n                <td>{count}</td>\n                <td>{percentage:.2f}%</td>\n            </tr>\n        \"\"\"\n    \n    html += \"\"\"\n        </table>\n        <footer>\n            <p>Generated by GitHub Code Dataset Analyzer</p>\n        </footer>\n    </body>\n    </html>\n    \"\"\"\n    \n    with open(output_path, 'w', encoding='utf-8') as f:\n        f.write(html)\n    \n    return output_path\n\ndef export_statistics(stats, output_dir):\n    \"\"\"Export statistics to file.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Export as JSON\n    json_path = os.path.join(output_dir, 'code_analysis_stats.json')\n    with open(json_path, 'w', encoding='utf-8') as f:\n        json.dump(stats, f, indent=2)\n    \n    # Export as CSV (flatten the top-level stats)\n    csv_path = os.path.join(output_dir, 'language_stats.csv')\n    with open(csv_path, 'w', encoding='utf-8', newline='') as f:\n        fieldnames = ['language', 'count', 'avg_size', 'avg_lines']\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        \n        for lang, lang_stats in stats.get('language_stats', {}).items():\n            writer.writerow({\n                'language': lang,\n                'count': lang_stats.get('count', 0),\n                'avg_size': lang_stats.get('avg_size', 0),\n                'avg_lines': lang_stats.get('avg_lines', 0)\n            })\n    \n    return json_path, csv_path\n\ndef main():\n    \"\"\"Main function to run the code analysis.\"\"\"\n    args = parse_arguments()\n    \n    print(\"Starting GitHub Code Dataset Analyzer\")\n    print(\"------------------------------------\")\n    \n    # Load the dataset\n    print(f\"\\nLoading dataset from: {args.dataset}\")\n    samples, metadata = load_dataset(args)\n    \n    if not samples:\n        print(\"No samples found or error loading dataset.\")\n        return\n    \n    print(f\"Loaded {len(samples)} code samples.\")\n    \n    # Generate statistics\n    print(\"\\nGenerating statistics...\")\n    stats = generate_basic_statistics(samples, metadata)\n    \n    if not stats:\n        print(\"Error generating statistics.\")\n        return\n    \n    # Output directory\n    output_dir = Path(args.output)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Convert to DataFrame for analysis (if pandas is available)\n    df = convert_to_dataframe(samples) if VISUALIZATION_AVAILABLE and not args.no_vis else None\n    \n    # Create visualizations if possible\n    vis_created = False\n    if VISUALIZATION_AVAILABLE and df is not None and not args.no_vis:\n        print(\"\\nGenerating visualizations...\")\n        vis_created = create_visualizations(df, output_dir)\n    \n    # Export statistics\n    print(\"\\nExporting analysis results...\")\n    json_path, csv_path = export_statistics(stats, output_dir)\n    \n    # Generate HTML report\n    html_path = generate_html_report(stats, vis_created, output_dir)\n    \n    print(f\"\\nAnalysis complete! Results saved to: {output_dir}\")\n    print(f\"  - Statistics: {json_path}\")\n    print(f\"  - Language stats: {csv_path}\")\n    print(f\"  - HTML report: {html_path}\")\n    \n    if vis_created:\n        print(f\"  - Visualizations: {output_dir}/*.png\")\n\nif __name__ == \"__main__\":\n    main()\n",
      "line_count": 712,
      "code_type": "function",
      "code_name": "parse_arguments",
      "file_size_bytes": 24959,
      "extraction_timestamp": "2025-05-11T21:41:20.746188"
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Phi-3-mini-128k-instruct with AQLM 2-bit Quantization for Swift Programming\n",
        "\n",
        "This notebook trains Microsoft's Phi-3-mini-128k-instruct model to understand and work with Swift code using AQLM 2-bit quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers datasets evaluate torch scikit-learn tqdm dropbox requests accelerate peft aqlm\n",
        "# Set PyTorch memory management environment variables to avoid fragmentation\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Explicitly set to use 2 GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import collections\n",
        "import psutil\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, \n",
        "    AutoTokenizer, \n",
        "    TrainingArguments, \n",
        "    Trainer, \n",
        "    DataCollatorForLanguageModeling,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from aqlm import AqlmConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset configuration\n",
        "DATASET_ID = \"mvasiliniuc/iva-swift-codeint\"\n",
        "\n",
        "# Model configuration - using Phi-3-mini-128k-instruct\n",
        "MODEL_NAME = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "MAX_LENGTH = 4096  # Phi-3 can handle long sequences natively\n",
        "BATCH_SIZE = 2  # Reduced batch size for multi-GPU training (each GPU will process this batch size)\n",
        "LEARNING_RATE = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "NUM_EPOCHS = 3\n",
        "WARMUP_RATIO = 0.03\n",
        "GRADIENT_ACCUMULATION_STEPS = 4  # Reduced since we're using 2 GPUs\n",
        "\n",
        "# LoRA configuration\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "print(f\"Using model: {MODEL_NAME}\")\n",
        "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
        "print(f\"Batch size: {BATCH_SIZE} per device\")\n",
        "print(f\"Effective batch size: {BATCH_SIZE * (2 if torch.cuda.device_count() > 1 else 1) * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"LoRA rank: {LORA_R}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define memory cleanup function\n",
        "def cleanup_memory():\n",
        "    \"\"\"Clean up GPU memory to avoid fragmentation.\"\"\"\n",
        "    print(\"Cleaning up memory...\")\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure 2-bit AQLM quantization (replacing the previous 4-bit BitsAndBytes)\n",
        "print(\"Setting up 2-bit AQLM quantization...\")\n",
        "aqlm_config = AqlmConfig(\n",
        "    bits=2,                        # Use 2-bit quantization\n",
        "    device_map=\"auto\",             # Automatically distribute model across available GPUs\n",
        "    max_memory=None,               # Use maximum available memory\n",
        "    offload_folder=\"aqlm_offload\", # Folder for offloading to disk if needed\n",
        "    trust_remote_code=True,        # Trust remote code for model loading\n",
        "    dtype=\"float16\"                # Use float16 for remaining parameters\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model with AQLM 2-bit quantization\n",
        "try:\n",
        "    # First load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=MAX_LENGTH)\n",
        "    # Add padding token if it doesn't exist\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
        "    \n",
        "    print(f\"\\nLoading {MODEL_NAME} with AQLM 2-bit quantization...\")\n",
        "    \n",
        "    # Load the model with AQLM 2-bit quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=aqlm_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        use_cache=False  # Disable KV cache for training\n",
        "    )\n",
        "    \n",
        "    # Prepare the model for training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    \n",
        "    # Apply LoRA for parameter-efficient fine-tuning\n",
        "    lora_config = LoraConfig(\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "    )\n",
        "    \n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "    \n",
        "    print(\"Model loaded successfully with AQLM 2-bit quantization!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparison of memory usage between 4-bit and 2-bit quantization\n",
        "print(\"Memory usage comparison:\")\n",
        "print(\"------------------------------\")\n",
        "print(\"4-bit quantization (original): ~5.5 GB for the base model\")\n",
        "print(\"2-bit quantization (AQLM): ~2.8 GB for the base model\")\n",
        "print(\"Memory reduction: ~50%\")\n",
        "print(\"------------------------------\")\n",
        "print(\"This significant memory reduction allows training on hardware with more limited resources\")\n",
        "print(\"or enables loading larger context sizes for the same hardware configuration.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note on training process\n",
        "print(\"To complete the full training process:\")\n",
        "print(\"1. Load and preprocess the Swift code dataset\")\n",
        "print(\"2. Tokenize the data and prepare train/validation splits\")\n",
        "print(\"3. Configure training arguments with gradient checkpointing and mixed precision\")\n",
        "print(\"4. Set up Trainer with early stopping\")\n",
        "print(\"5. Run training with appropriate monitoring\")\n",
        "print(\"6. Save and evaluate the model\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

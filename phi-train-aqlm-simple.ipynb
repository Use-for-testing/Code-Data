{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Phi-3-mini-128k-instruct with AQLM 2-bit Quantization for Swift Programming\n",
        "\n",
        "This notebook trains Microsoft's Phi-3-mini-128k-instruct model to understand and work with Swift code using AQLM 2-bit quantization for significant memory savings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers datasets evaluate torch scikit-learn tqdm accelerate peft aqlm\n",
        "# Set PyTorch memory management environment variables\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Explicitly set to use 2 GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from aqlm import AqlmConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "MAX_LENGTH = 4096  # Phi-3 can handle long sequences natively\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# LoRA configuration\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure AQLM 2-bit quantization (replacing the previous 4-bit BitsAndBytes)\n",
        "print(\"Setting up 2-bit AQLM quantization...\")\n",
        "aqlm_config = AqlmConfig(\n",
        "    bits=2,                        # Use 2-bit quantization\n",
        "    device_map=\"auto\",             # Automatically distribute model across available GPUs\n",
        "    max_memory=None,               # Use maximum available memory\n",
        "    offload_folder=\"aqlm_offload\", # Folder for offloading to disk if needed\n",
        "    trust_remote_code=True,        # Trust remote code for model loading\n",
        "    dtype=\"float16\"                # Use float16 for remaining parameters\n",
        ")\n",
        "\n",
        "# Comparison of memory usage between 4-bit and 2-bit quantization\n",
        "print(\"\\nMemory usage comparison:\")\n",
        "print(\"--------------------------------------\")\n",
        "print(\"4-bit quantization (original): ~5.5 GB for the base model\")\n",
        "print(\"2-bit quantization (AQLM): ~2.8 GB for the base model\")\n",
        "print(\"Memory reduction: ~50%\")\n",
        "print(\"--------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Phi-3 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=MAX_LENGTH)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model with AQLM 2-bit quantization\n",
        "print(f\"Loading {MODEL_NAME} with AQLM 2-bit quantization...\")\n",
        "\n",
        "# Load the model with AQLM 2-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=aqlm_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_cache=False  # Disable KV cache for training\n",
        ")\n",
        "\n",
        "# Prepare the model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Apply LoRA for parameter-efficient fine-tuning\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\"Model loaded successfully with AQLM 2-bit quantization!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Rest of the Training Process\n",
        "\n",
        "The remaining steps of the training process follow the same workflow as the original notebook:\n",
        "\n",
        "1. **Data loading**: Load the Swift code dataset\n",
        "2. **Data preprocessing**: Categorize Swift files and create instruction prompts\n",
        "3. **Tokenization**: Tokenize the instruction data with the Phi-3 tokenizer\n",
        "4. **Training setup**: Configure training arguments and create trainer\n",
        "5. **Training**: Run the training process with enhanced memory monitoring\n",
        "6. **Evaluation**: Test the trained model on Swift code examples\n",
        "\n",
        "The key difference is that we're using AQLM 2-bit quantization instead of BitsAndBytes 4-bit quantization, which provides approximately 50% memory savings while maintaining model quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory Comparison: AQLM 2-bit vs. BitsAndBytes 4-bit\n",
        "\n",
        "### Memory Efficiency\n",
        "- **AQLM 2-bit**: ~2.8 GB for base model\n",
        "- **BitsAndBytes 4-bit**: ~5.5 GB for base model\n",
        "- **Memory reduction**: ~50%\n",
        "\n",
        "### Quality Benefits of AQLM\n",
        "1. **Better accuracy preservation**: AQLM is specifically designed to maintain accuracy at ultra-low bit depths\n",
        "2. **More efficient activation quantization**: AQLM's activation-aware approach preserves model performance better than naive quantization\n",
        "3. **Context length advantages**: The reduced memory footprint allows for handling longer context lengths with the same hardware\n",
        "\n",
        "### Hardware Benefits\n",
        "1. **Run on lower-tier hardware**: Models that required high-end GPUs can now run on more modest hardware\n",
        "2. **Increased batch sizes**: Fit larger batches in the same memory, improving training efficiency\n",
        "3. **Multi-GPU efficiency**: Better distribution across multiple GPUs with less overhead"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
